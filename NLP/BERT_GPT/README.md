# Natural Language Processing (NLP) Lecture Notes

This repository contains important lecture notes focused on advanced Natural Language Processing (NLP) topics such as transformers, BERT, and GPT models. These notes provide theoretical insights into the models that power modern NLP tasks.

## Contents

1. **Lecture 11 - Transformers and BERT.pdf**
2. **Lecture 12 - BERT and GPT.pdf**

---

## 1. Lecture 11 - Transformers and BERT

### Overview

This lecture focuses on the transformer architecture, highlighting its importance in NLP tasks. It introduces the concept of self-attention and explains how transformers surpass traditional sequence models like RNNs.

### Key Topics

- **Transformers**: Encoder-decoder architecture and self-attention mechanisms.
- **Attention Mechanisms**: How attention improves the performance of sequence models.
- **BERT**: Introduction to the Bidirectional Encoder Representations from Transformers and its applications in language tasks.

### How to Use

Read `Lecture 11 - Transformers and BERT.pdf` to understand how transformers and BERT are used for complex NLP tasks, including machine translation and text classification.

---

## 2. Lecture 12 - BERT and GPT

### Overview

This lecture dives into the workings of BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pretrained Transformer). It explains how these models are trained and their use cases in various NLP tasks.

### Key Topics

- **BERT**: How BERT uses masked language modeling to capture bidirectional context and its practical applications.
- **GPT**: Understanding auto-regressive models and their role in generating text.
- **Pre-trained Transformer Models**: Overview of available pre-trained models and their use in specific NLP tasks.

### How to Use

Read `Lecture 12 - BERT and GPT.pdf` for insights into how BERT and GPT models are trained and applied to tasks like text generation, sentiment analysis, and more.

---
