# Natural Language Processing (NLP) Lab Exercises

This repository contains a collection of NLP lab exercises that cover various aspects of language modeling, text processing, and classification. The exercises demonstrate the application of NLP techniques using popular tools like Python and Jupyter Notebooks.

## Contents

1. **Lab2_N-grams.ipynb**
2. **Lab2_Naive_Bayes.ipynb**
3. **Lecture_2_LanguageModeling.pdf**

---

## 1. Lab2_N-grams.ipynb

### Overview

This notebook covers two exercises:

- **Exercise 1**: N-grams and text processing using a given dataset.
- **Exercise 2**: Applying N-gram techniques to a new dataset.

### Key Features

- **Exercise 1**:
  - Generates N-grams from a text dataset.
  - Demonstrates text preprocessing techniques (e.g., tokenization, removing stopwords).
- **Exercise 2**:
  - A new dataset is loaded and preprocessed for generating N-grams.
  - Applies the same techniques as Exercise 1 to explore new text data.

### How to Run

Run the notebook `NLP_Lab2_Exercise1&2_BhaveshWaghela.ipynb` in Jupyter to replicate the N-gram generation and text preprocessing steps.

### Output

- N-grams generated from the datasets.
- Preprocessed text data.

---

## 2. Lab2_Naive_Bayes.ipynb

### Overview

This notebook focuses on text classification using the Naive Bayes algorithm. The goal is to categorize text data into predefined categories based on its content.

### Key Features

- **Text Classification**:
  - Uses the Naive Bayes classifier to predict the category of text data.
  - Applies preprocessing techniques to prepare the data for classification.
  - Evaluates the performance of the classifier with accuracy metrics.

### How to Run

Run the notebook `NLP_Lab2_Exercise3_BhaveshWaghela.ipynb` in Jupyter to perform text classification using Naive Bayes.

### Output

- Predicted categories for the text data.
- Evaluation metrics to assess the model's performance.

---

## 3. Lecture_2_LanguageModeling.pdf

### Overview

This document provides an in-depth overview of language modeling techniques, including probabilistic models, N-gram models, and their importance in NLP applications.

### Key Topics

- **Probabilistic Language Models**: Assigning probabilities to word sequences and generating text.
- **N-gram Models**: Using N-grams to predict the next word in a sequence based on the previous N-1 words.
- **Markov Assumption**: The concept that future events depend on a limited history of past events.
- **Text Processing**: Tokenization, handling punctuation, capitalization, and more.

### How to Use

Read the `Lecture_2_LanguageModeling.pdf` document to gain a theoretical understanding of language modeling, which complements the practical exercises in the notebooks.

---
