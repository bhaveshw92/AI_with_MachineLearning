# Natural Language Processing (NLP) Projects, Labs, and Lectures

This repository contains a comprehensive collection of assignments, labs, and lecture notes related to Natural Language Processing (NLP). It covers a wide range of topics, from basic NLP concepts to advanced deep learning models like Word2Vec and transformers. The repository is structured into different folders, each focusing on specific topics or tasks in NLP.

## Repository Structure

The repository is organized as follows:

### 1. **Assignments**

Contains all the NLP-related assignments that demonstrate practical applications of concepts learned in lectures. Each assignment folder includes Jupyter notebooks and necessary datasets.

#### Contents:

- **Assignment 1**: Preprocessing Twitter Data
- **Assignment 2**: Sentiment Analysis on YouTube Comments
- **Assignment 3**: Text Classification using Naive Bayes
- **Assignment 4**: Named Entity Recognition and TF-IDF

### 2. **Labs**

The labs focus on hands-on learning of various NLP tools and libraries. Each lab includes exercises on fundamental NLP tasks such as tokenization, word embeddings, and model training using popular libraries like PyTorch and Gensim.

#### Contents:

- **Lab 1**: N-grams and Text Processing
- **Lab 2**: Word2Vec using Gensim
- **Lab 3**: Named Entity Recognition (NER)
- **Lab 4**: TF-IDF and Text Vectorization
- **Lab 5**: PyTorch Basics and Neural Network Implementation for NLP

### 3. **Lectures**

This section includes lecture notes and tutorials that provide theoretical foundations for the NLP techniques applied in the assignments and labs. These resources cover essential topics such as neural networks, Word2Vec, transformers, and PyTorch.

#### Contents:

- **Lecture 1**: Introduction to NLP and Language Models
- **Lecture 2**: Chunking and Word Embeddings
- **Lecture 3**: Neural Networks and Deep Learning
- **Lecture 4**: PyTorch Basics and Implementation

## Key Topics Covered

- **Natural Language Processing Basics**: Tokenization, stopword removal, text normalization.
- **Text Preprocessing**: N-grams, TF-IDF, and vectorization techniques.
- **Word Embeddings**: Word2Vec, GloVe, and fastText.
- **Text Classification**: Naive Bayes, Logistic Regression, and Neural Networks.
- **Deep Learning for NLP**: Feedforward neural networks, transformers, BERT, and GPT models.
- **PyTorch for NLP**: Implementing neural networks and training models using PyTorch.

## Getting Started

Each folder contains a `README.md` with detailed instructions on how to run the corresponding notebooks or exercises. Be sure to check the individual folders for specific guidelines and requirements for each lab or assignment.

## External Resources

Some large datasets and models are linked externally to ensure smooth access:

- [Google Drive link to datasets](https://drive.google.com/drive/folders/1J4Cm6XyltC5Pb_r6hvAE1xmEyJupW5bv?usp=sharing)

## License

This repository is licensed under the MIT License.

---

Feel free to explore the repository and check out the individual assignments, labs, and lecture materials for a complete learning experience in Natural Language Processing.
