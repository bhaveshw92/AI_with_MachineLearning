# Natural Language Processing with RNN

This repository contains a collection of tutorials, lecture notes, and Lab Notebooks focused on using RNN, LSTMs and Text Generation for Natural Language Processing (NLP).

## Contents

### 1. Sentiment Analysis with LSTMs

This project demonstrates how to perform sentiment analysis using Long Short-Term Memory (LSTM) networks. The notebook `Lab9_SentimentAnalysis_with_LSTMs.ipynb` contains all the necessary steps to preprocess data, build, train, and evaluate an LSTM model for sentiment analysis.

## Prerequisites

Before running the notebook, ensure you have the following installed:

- Python 3.x
- Jupyter Notebook or Jupyter Lab
- Required Python libraries (listed below)

## Notebook Contents

### 1. Introduction

- Overview of sentiment analysis and LSTM networks.

### 2. Data Preprocessing

- Loading the dataset.
- Cleaning and preprocessing the text data.
- Tokenizing and padding sequences.

### 3. Building the LSTM Model

- Defining the architecture of the LSTM network.
- Compiling the model with appropriate loss function and optimizer.

### 4. Training the Model

- Splitting the data into training and validation sets.
- Training the LSTM model on the training data.
- Monitoring the training process using validation data.

### 5. Evaluating the Model

- Evaluating the model's performance on test data.
- Visualizing training and validation loss and accuracy.

### 6. Making Predictions

- Using the trained model to make predictions on new data.
- Interpreting the results of the predictions.

## Usage

To run the notebook, open it in Jupyter Notebook or Jupyter Lab and execute the cells sequentially. Ensure that you have downloaded the dataset and placed it in the appropriate directory as specified in the notebook.

## Dataset

The dataset used for this project should be a labeled dataset suitable for sentiment analysis. Common datasets include the IMDb movie reviews dataset or the Twitter sentiment analysis dataset. Ensure that the dataset is preprocessed as required by the notebook.

## Results

The notebook will output various metrics and visualizations to help you understand the performance of the LSTM model. You will see plots of training and validation loss and accuracy, as well as confusion matrices and classification reports.

---

### 2. Text Generation with LSTMs

This project demonstrates how to perform text generation using Long Short-Term Memory (LSTM) networks. The notebook `Lab9_TextGen.ipynb` contains all the necessary steps to preprocess data, build, train, and generate text using an LSTM model.

## Notebook Contents

### 1. Introduction

- Overview of text generation and LSTM networks.

### 2. Data Loading and Exploration

- Loading the dataset.
- Exploring the dataset to understand its structure and content.

### 3. Data Preprocessing

- Cleaning and preprocessing the text data.
- Tokenizing and padding sequences.

### 4. Building the LSTM Model

- Defining the architecture of the LSTM network.
- Compiling the model with appropriate loss function and optimizer.

### 5. Training the Model

- Splitting the data into training and validation sets.
- Training the LSTM model on the training data.
- Monitoring the training process using validation data.

### 6. Generating Text

- Using the trained model to generate new text sequences.
- Fine-tuning the generation process to improve the quality of the generated text.

## Usage

To run the notebook, open it in Jupyter Notebook or Jupyter Lab and execute the cells sequentially. Ensure that you have downloaded the dataset and placed it in the appropriate directory as specified in the notebook.

## Dataset

The dataset used for this project should be a text corpus suitable for text generation tasks. Ensure that the dataset is preprocessed as required by the notebook.

## Results

The notebook will output various metrics and visualizations to help you understand the performance of the LSTM model. You will see plots of training and validation loss and accuracy, as well as examples of generated text.

---

### 3. Lecture Slides for RNN and LSTMs at Lecture9_RNN_LSTMs.pdf
