{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcqQ-ACY0W18"
      },
      "source": [
        "# **PyTorch Tutorial**\n",
        "\n",
        "### **Please make a copy of this notebook to edit.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyAGzKT60rI_"
      },
      "source": [
        "Hello! In this notebook, we will introduce PyTorch, an essential machine learning framework for Python. Documentation for the package can be found [here](https://pytorch.org/docs/stable/index.html). Additional resources on how to use PyTorch can be found all around the Internet, though we strongly encourage you to fully explore all of the materials on the PyTorch website prior to moving to external sources.\n",
        "\n",
        "This notebook assumes that you have already installed PyTorch on your device. If you have not yet installed PyTorch, please follow the instructions [here](https://pytorch.org/get-started/locally/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPoJkheu2Alr"
      },
      "source": [
        "<a name=\"outline\"></a>\n",
        "## **Outline**\n",
        "1. [What is PyTorch and Why Do We Use It?](#part1)\n",
        "2. [Working with Tensors](#part2)\n",
        "3. [Word Embeddings](#part3)\n",
        "4. [Building a FFNN](#part4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqZlRFkRqVwD"
      },
      "source": [
        "<a name=\"part1\"></a>\n",
        "[[^^^]](#outline)\n",
        "## **Part 1: What is PyTorch and Why Do We Use It?**\n",
        "\n",
        "PyTorch is a robust deep learning framework used by academics and researchers alike to advance scientific discoveries in a number of fields. Originally developed by researchers at Facebook (now Meta AI), PyTorch offers a straightforward pathway into the world of machine learning.\n",
        "\n",
        "We are using PyTorch in this class for a number of reasons. Firstly, PyTorch is very \"pythonic\" and easy-to-learn compared to some other machine learning frameworks. Secondly, the rich documentation and expansive community allow for less frustrations when it comes to developing and debugging models. Thirdly, there are a number of behind-the-scenes operations that occur (namely, dynamic computation graphing) which helps us optimize our models and their training.\n",
        "\n",
        "For more information on key features of PyTorch, please consider the following links:\n",
        "\n",
        "1. [PyTorch Overview (Video)](https://www.youtube.com/watch?v=RwaVqvZ3xo8)\n",
        "2. [Advantages of PyTorch in Comparison to Tensorflow (Video)](https://www.youtube.com/watch?v=nbJ-2G2GXL0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BocXggx_qdLz"
      },
      "source": [
        "<a name=\"part2\"></a>\n",
        "[[^^^]](#outline)\n",
        "## **Part 2: Working with Tensors**\n",
        "\n",
        "In this section, we'll go over the basic properties and operations on tensors. This is by no means an exhaustive list. If you _really_ want to get into the details, please see the PyTorch documentation [here](https://pytorch.org/docs/stable/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmrQPGTau9Wt"
      },
      "source": [
        "#### **2.1: Tensor Basics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EgYlBwYfqgp4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x231883fb850>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Setting a seed so we can all get the same numbers\n",
        "torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_kZTeDYqNfF"
      },
      "source": [
        "Tensors are very easy to initialize! One way of doing so is by converting a list of numerical values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hVms5Yjkql_3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 0., 10., 11.])\n"
          ]
        }
      ],
      "source": [
        "# Converting list to tensor\n",
        "data = [0, 10, 11]\n",
        "t1 = torch.Tensor(data)\n",
        "print(t1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T9JE-ZJjqzmn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<built-in method type of Tensor object at 0x0000023181DBA030>\n",
            "torch.Size([3])\n"
          ]
        }
      ],
      "source": [
        "# Examining aspects of our first tensor:\n",
        "print(t1.type)\n",
        "print(t1.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eygRcyIIqh9p"
      },
      "source": [
        "You can also convert numpy arrays to tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aK5TbMzkq4Zo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.5058, 0.8798, 0.2380],\n",
            "        [0.8197, 0.9978, 0.4823]], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# A tensor of more complex dimensionality:\n",
        "data_2_np = np.random.rand(2,3)\n",
        "\n",
        "t2 = torch.from_numpy(data_2_np)\n",
        "print(t2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0CA9rBisrAML"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3])\n"
          ]
        }
      ],
      "source": [
        "# Examining shape\n",
        "print(t2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qul8pxuZqkrZ"
      },
      "source": [
        "However, _note the following_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CJ4QD-xnqnbF"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "too many dimensions 'str'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m data_txt \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbarked\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m t_txt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(data_txt)\n",
            "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
          ]
        }
      ],
      "source": [
        "data_txt = ['The', 'dog', 'barked']\n",
        "\n",
        "t_txt = torch.Tensor(data_txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LboOFaawsglv"
      },
      "source": [
        "Any idea what's going on in the cell above? \n",
        "\n",
        "torch.tensor() while it can't handle string data\n",
        "\n",
        "Finally, some other very useful ways to initialize tensors: rand, ones, and zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Ytd2KEgmr_WK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2961, 0.5166],\n",
            "        [0.2517, 0.6886],\n",
            "        [0.0740, 0.8665]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "# Some ways to initialize tensors:\n",
        "shape = (3, 2,)\n",
        "t_rand = torch.rand(shape)\n",
        "t_ones = torch.ones(shape)\n",
        "t_zeros = torch.zeros(shape)\n",
        "\n",
        "print(t_rand)\n",
        "print(t_ones)\n",
        "print(t_zeros)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRuq9ny6vBmP"
      },
      "source": [
        "#### **2.2: Operations on Tensors**\n",
        "\n",
        "While tensors are not lists, they do carry many similar properties of lists. You can slice them with indices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ZEW6hkVErsDC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First row of t2: tensor([0.5058, 0.8798, 0.2380], dtype=torch.float64)\n",
            "Second column only of t2: tensor([0.8798, 0.9978], dtype=torch.float64)\n",
            "Second row, third item of t2: tensor(0.4823, dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# First row\n",
        "print('First row of t2:', (t2[0]))\n",
        "\n",
        "# Second column only\n",
        "print('Second column only of t2:', t2[:, 1])\n",
        "\n",
        "# Second row, third item\n",
        "print('Second row, third item of t2:', t2[1, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjPAzulRtMdD"
      },
      "source": [
        "... and you can combine them (two different ways!):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Dh8uFuOpuvrD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.],\n",
            "        [4., 5., 6.],\n",
            "        [7., 8., 9.]])\n",
            "torch.Size([4, 3])\n",
            "\n",
            "tensor([[[1., 2., 3.],\n",
            "         [4., 5., 6.]],\n",
            "\n",
            "        [[4., 5., 6.],\n",
            "         [7., 8., 9.]]])\n",
            "torch.Size([2, 2, 3])\n"
          ]
        }
      ],
      "source": [
        "# Two ways of combining tensors:\n",
        "t3 = torch.Tensor([[1,2,3],\n",
        "                   [4,5,6]])\n",
        "\n",
        "t4 = torch.Tensor([[4,5,6],\n",
        "                   [7,8,9]])\n",
        "\n",
        "# Combining along a given dimension with torch.cat\n",
        "t_cat = torch.cat((t3, t4), dim=0) # Not the Tompkins Consolidated Area Transport\n",
        "print(t_cat)\n",
        "print(t_cat.shape)\n",
        "print()\n",
        "\n",
        "# Combining along a new dimension with torch.stack\n",
        "t_stack = torch.stack((t3, t4), dim=0)\n",
        "print(t_stack)\n",
        "print(t_stack.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfcEDd1EtTGE"
      },
      "source": [
        "Note the differences with the two methods above. Even though we are combining the same two tensors, we get output tensors of different dimensionalities. `torch.cat` takes two (or more) tensors and combines them along the specified dimension (default = 0). `torch.stack` takes two (or more) tensors and combines them along a NEW dimension. As its name suggests, `torch.stack` places two tensors on top of one another in a different dimension. Cool stuff! For more information on `torch.cat`, go [here](https://pytorch.org/docs/stable/generated/torch.cat.html); for more information on `torch.stack`, go [here](https://pytorch.org/docs/stable/generated/torch.stack.html).\n",
        "\n",
        "We can also sum across multiple dimensions (default = 0):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "K-inq2Gbv0QP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "\n",
            "tensor([5., 7., 9.])\n",
            "\n",
            "tensor([ 6., 15.])\n"
          ]
        }
      ],
      "source": [
        "# Summing\n",
        "print(t3)\n",
        "print()\n",
        "\n",
        "# Across columns\n",
        "print(t3.sum(dim=0))\n",
        "print()\n",
        "\n",
        "# Across rows\n",
        "print(t3.sum(dim=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps4AbfgGvuiw"
      },
      "source": [
        "... and do do element multiplication:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "29MmSboTxZTl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 4., 10., 18.],\n",
            "        [28., 40., 54.]])\n",
            "tensor([[ 4., 10., 18.],\n",
            "        [28., 40., 54.]])\n"
          ]
        }
      ],
      "source": [
        "# Element multiplication\n",
        "t_em_1 = t3 * t4\n",
        "t_em_2 = t3.mul(t4)\n",
        "\n",
        "print(t_em_1)\n",
        "print(t_em_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkY3M8DMvy6N"
      },
      "source": [
        "... along with matrix multiplication:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "0PE_IlhNx8Cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[14., 32.],\n",
            "        [32., 77.]])\n",
            "tensor([[14., 32.],\n",
            "        [32., 77.]])\n"
          ]
        }
      ],
      "source": [
        "# Matrix multiplication\n",
        "t_mm_1 = t3 @ t3.T\n",
        "t_mm_2 = t3.matmul(t3.T)\n",
        "\n",
        "print(t_mm_1)\n",
        "print(t_mm_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY09wfQNv3r2"
      },
      "source": [
        "Note that there are multiple ways of completing this operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f87bUDlzPyo9"
      },
      "source": [
        "<a name=\"part3\"></a>\n",
        "[[^^^]](#outline)\n",
        "## **Part 3: Word Embeddings**\n",
        "\n",
        "Now that we have become familiar with the basic concepts of tensors and their various properties, let's begin moving into NLP territory. As the title of this section indicates, we will begin our journey with word embeddings, an essential concept in NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "0K_xYADKOLep"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-bKrmCdY1bN"
      },
      "source": [
        "As described in previous lectures, basic word embeddings serve as a computational approach to the distributional hypothesis: words that are similar to each other occur in similar contexts. These embeddings can then inform our model of various \"semantic\" features that are present in language data.\n",
        "\n",
        "To provide a more practical example of how embeddings operate, consider the following nonsense sentences concerned with various kinds of aviation-related things:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Xo4Gh2frY3dY"
      },
      "outputs": [],
      "source": [
        "tiny_dataset = ['The helicopter engine broke through the rocket sky.',\n",
        "                'The helicopter engine soared above the drone sky.',\n",
        "                'The drone engine broke through the rocket sky'\n",
        "                'The drone engine soared above the helicopter sky',\n",
        "                'The drone engine knocked out the drone sky',\n",
        "                'The rocket engine broke through the helicopter sky'\n",
        "                'The rocket engine soared above the helicopter sky',\n",
        "                'The rocket engine knocked out the helicopter sky',\n",
        "                'The rocket engine shredded below the drone sky',\n",
        "                'The bee wings broke through the goose sky.',\n",
        "                'The bee wings soared above the goose sky.',\n",
        "                'The bee wings knocked out the eagle sky.',\n",
        "                'The eagle wings broke through the goose sky.',\n",
        "                'The eagle wings soared above the eagle sky.',\n",
        "                'The eagle wings knocked out the bee sky.',\n",
        "                'The goose wings broke through the goose sky.',\n",
        "                'The goose wings soared above the eagle sky.',\n",
        "                'The jet wings soared above the bee sky.',\n",
        "                'The jet engine broke through the jet sky.']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzNVbfSHY8qk"
      },
      "source": [
        "Wow -- that's a _lot_ of co-occurrences! Let's use this tiny, repetitive dataset to build very simple 2-dimensional embeddings based on counts.\n",
        "\n",
        "For now, let's focus our attention on the machines (\"helicopter\", \"drone\", \"rocket\") and two contexts (\"engine\" and \"sky\"). To save some time, the following table shows the number of times that each machine appears next to \"engine\" and \"sky\":\n",
        "\n",
        "\n",
        "|            | context: engine | context: sky |\n",
        "|------------|-----------------|--------------|\n",
        "| helicopter | 2               | 4            |\n",
        "| drone      | 3               | 3            |\n",
        "| rocket     | 4               | 2            |\n",
        "\n",
        "\n",
        "For the purposes of this lecture, we will ignore that each machine is always followed by \"the\". I'll leave it up to you how this may (or may not) affect our embedding representation!\n",
        "\n",
        "\n",
        "In this example, we can take each context in the table above as a dimension (x=\"engine\", y=\"sky\") and plot the coordinates for each machine according to its counts in each dimension:\n",
        "\n",
        "![2d_embeddings](https://drive.google.com/uc?export=view&id=1HSmuy4Epym_dBBzXQScTs5LGZz7ScWZU)\n",
        "\n",
        "In the graph above, we see that \"helicopter\" and \"drone\" are as close as \"rocket\" and \"drone\" (aka θ1 = θ2, where θ = cosine similarity. More on cosine similarity later!). However, \"helicopter\" and \"rocket\" appear to be further apart.\n",
        "\n",
        "\n",
        "What if we consider more dimensions? Let's add \"wings\" to our contexts as a third dimension. For our words, let's add \"bee\", \"eagle\", \"goose\", and \"jet\". Our new table looks something like this:\n",
        "\n",
        "|            | context: engine | context: sky | context: wings |\n",
        "|------------|-----------------|--------------|----------------|\n",
        "| helicopter | 2               | 4            | 0              |\n",
        "| drone      | 3               | 3            | 0              |\n",
        "| rocket     | 4               | 2            | 0              |\n",
        "| bee        | 0               | 2            | 3              |\n",
        "| goose      | 0               | 4            | 2              |\n",
        "| eagle      | 0               | 3            | 3              |\n",
        "| jet        | 1               | 1            | 1              |\n",
        "\n",
        "\n",
        "We can then use a three-dimensional plot with axes x=\"engine\", y=\"sky\", and z=\"wings\" to plot the counts of each word in context:\n",
        "\n",
        "\n",
        "![3d_embeddings](https://drive.google.com/uc?export=view&id=1crOuM7Zqv8iFqew8-fUDFAIoDnHKQapP)\n",
        "\n",
        "Above, we can see a clear difference in the vector representations between the words that co-occur with \"wings\" and those that do not. Cool stuff!\n",
        "\n",
        "\n",
        "**An important note:** For the purposes of explication, we map a clear relationship in the two examples above: each context is a dimension, and each instance of a word in that context serves as a count, which in turn contributes to the word's exact vector representation. However, these vectors *do not* require such a clear mapping. Word embeddings with *n*-dimensions do not require an *n* that is equal to the total number of contexts. Instead, we can reduce (or increase) *n* to create more abstract, less interpretable dimensions. We will discuss this in the following sections!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykJGEvKjOMKO"
      },
      "source": [
        "#### **3.1 PyTorch embeddings**\n",
        "Okay, enough with the background!\n",
        "\n",
        "Previously, we mentioned that PyTorch tries to make your life easier. Let's get into the first example of it doing so! Consider the sentence \"Hello class\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "kP0sPxtTP_91"
      },
      "outputs": [],
      "source": [
        "# A very basic example:\n",
        "word_to_ix = {\"hello\": 0, \"class\": 1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC56LR8_2mYs"
      },
      "source": [
        "To initialize embeddings, we can use `nn.Embedding` and specify two dimensions: the number of embeddings you need, and the dimensionality of the embeddings. Let's create embeddings of dimensions 2x5 (2 words in the vocab, each embedding has 5 dimensions):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "J42h7fIk2tjz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1])\n",
            "tensor([[ 0.9687,  0.0814,  0.3012, -0.2114,  0.3071]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
        "\n",
        "# Looking at class:\n",
        "lookup_tensor = torch.tensor([word_to_ix[\"class\"]], dtype=torch.long)     # Note that this is blank tensor of the words ids:\n",
        "print(lookup_tensor)\n",
        "\n",
        "# And then the embeddings initialize\n",
        "hello_embed = embeds(lookup_tensor)\n",
        "print(hello_embed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nlz2MXd0XArp"
      },
      "source": [
        "Note that these are initialized embeddings are initialized randomly at first! If we want them to better approximate the semantics of our data, then we will need to undergo model training. At the moment, however, they will appear to be pretty random."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6GCPo1q4XjB"
      },
      "source": [
        "We now know how to create PyTorch embeddings. Woo hoo! However, how can we _analyze_ them? One method of doing so is checking the cosine similarity between two embeddings (see lecture slides for more information).\n",
        "\n",
        "Luckily for us, PyTorch has a built-in CosineSimilarity function: `nn.CosineSimilarity`. While we would traditionally apply this function to word embeddings, _we will not do so here_ (don't worry, we will soon!), as we do not have trained embeddings. As such, we will use tensors to demonstrate cosine similarity. Let's initialize five tensors (feel free to mess around with these numbers to experiment with cosine similarity):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "bee4Ic53YrJo"
      },
      "outputs": [],
      "source": [
        "# PyTorch's built-in Cosine Similarity function:\n",
        "c1 = torch.Tensor([4,6,8])\n",
        "c2 = torch.Tensor([4,7,8])\n",
        "c3 = torch.Tensor([0,12,24])\n",
        "c4 = torch.Tensor([-59, 78, -14])\n",
        "c5 = torch.Tensor([16, 24, 32])\n",
        "cs = nn.CosineSimilarity(dim=0, eps=1e-6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FMCaE_88gfn"
      },
      "source": [
        "How do you think these will pattern? Which will be most similar? Which will be the least similar?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "bXTZRN2x8hsx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c1, c2: tensor(0.9973)\n",
            "c1, c3: tensor(0.9135)\n",
            "c1, c4: tensor(0.1128)\n",
            "c1, c5: tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "# c1 compared to c2\n",
        "print('c1, c2:', cs(c1, c2))\n",
        "\n",
        "# c1 compared to c3\n",
        "print('c1, c3:', cs(c1, c3))\n",
        "\n",
        "# c1 compared to c4\n",
        "print('c1, c4:', cs(c1, c4))\n",
        "\n",
        "# c1 compared to c5\n",
        "print('c1, c5:', cs(c1, c5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ_ZEo8b3FfP"
      },
      "source": [
        "For more information on `nn.Embedding`, look [here](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html). For more information on `nn.CosineSimilarity`, look [here](https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl56HMVQQClj"
      },
      "source": [
        "#### **3.2 An Example of Pre-trained Embeddings: GloVe Embeddings**\n",
        "\n",
        "While it is essential to know how to train your own embeddings (as pertains to whatever task you are trying to complete), a number of pre-trained embeddings exist. In this tutorial, we will be using the GloVe embeddings as outlined in [Pennington et al. (2014)](https://nlp.stanford.edu/pubs/glove.pdf). Documentation for GloVe embeddings can be found [here](https://nlp.stanford.edu/projects/glove/).\n",
        "\n",
        "First, we must download the GloVe embeddings. The following block of code will do this for you, but it may take some time to run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchtext\n",
            "  Downloading torchtext-0.17.1-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: tqdm in c:\\users\\bhave\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torchtext) (4.66.2)\n",
            "Requirement already satisfied: requests in c:\\users\\bhave\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torchtext) (2.31.0)\n",
            "Collecting torch==2.2.1 (from torchtext)\n",
            "  Downloading torch-2.2.1-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\bhave\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torchtext) (1.26.0)\n",
            "Collecting torchdata==0.7.1 (from torchtext)\n",
            "  Downloading torchdata-0.7.1-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\bhave\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torch==2.2.1->torchtext) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\bhave\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torch==2.2.1->torchtext) (4.8.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\bhave\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torch==2.2.1->torchtext) (1.11.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\bhave\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torch==2.2.1->torchtext) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\bhave\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torch==2.2.1->torchtext) (3.1.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\bhave\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torch==2.2.1->torchtext) (2024.2.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in c:\\users\\bhave\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torchdata==0.7.1->torchtext) (1.26.18)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bhave\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->torchtext) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhave\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhave\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->torchtext) (2023.7.22)\n",
            "Requirement already satisfied: colorama in c:\\users\\bhave\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bhave\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from jinja2->torch==2.2.1->torchtext) (2.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\bhave\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from sympy->torch==2.2.1->torchtext) (1.3.0)\n",
            "Downloading torchtext-0.17.1-cp311-cp311-win_amd64.whl (1.9 MB)\n",
            "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.0/1.9 MB 1.3 MB/s eta 0:00:02\n",
            "   ---- ----------------------------------- 0.2/1.9 MB 2.8 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 1.7/1.9 MB 13.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.9/1.9 MB 12.4 MB/s eta 0:00:00\n",
            "Downloading torch-2.2.1-cp311-cp311-win_amd64.whl (198.6 MB)\n",
            "   ---------------------------------------- 0.0/198.6 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.3/198.6 MB 42.9 MB/s eta 0:00:05\n",
            "    --------------------------------------- 2.8/198.6 MB 35.7 MB/s eta 0:00:06\n",
            "   - -------------------------------------- 5.7/198.6 MB 45.4 MB/s eta 0:00:05\n",
            "   - -------------------------------------- 8.2/198.6 MB 47.5 MB/s eta 0:00:05\n",
            "   -- ------------------------------------- 10.6/198.6 MB 50.4 MB/s eta 0:00:04\n",
            "   -- ------------------------------------- 13.3/198.6 MB 54.4 MB/s eta 0:00:04\n",
            "   --- ------------------------------------ 15.5/198.6 MB 46.7 MB/s eta 0:00:04\n",
            "   --- ------------------------------------ 18.5/198.6 MB 54.7 MB/s eta 0:00:04\n",
            "   ---- ----------------------------------- 21.3/198.6 MB 54.4 MB/s eta 0:00:04\n",
            "   ---- ----------------------------------- 24.1/198.6 MB 54.4 MB/s eta 0:00:04\n",
            "   ----- ---------------------------------- 26.3/198.6 MB 54.4 MB/s eta 0:00:04\n",
            "   ----- ---------------------------------- 28.7/198.6 MB 54.4 MB/s eta 0:00:04\n",
            "   ------ --------------------------------- 31.4/198.6 MB 54.7 MB/s eta 0:00:04\n",
            "   ------ --------------------------------- 33.3/198.6 MB 59.5 MB/s eta 0:00:03\n",
            "   ------- -------------------------------- 35.4/198.6 MB 46.7 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 38.3/198.6 MB 50.4 MB/s eta 0:00:04\n",
            "   -------- ------------------------------- 41.1/198.6 MB 50.4 MB/s eta 0:00:04\n",
            "   -------- ------------------------------- 43.1/198.6 MB 46.9 MB/s eta 0:00:04\n",
            "   --------- ------------------------------ 45.1/198.6 MB 50.4 MB/s eta 0:00:04\n",
            "   --------- ------------------------------ 47.3/198.6 MB 46.7 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 50.5/198.6 MB 46.7 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 52.8/198.6 MB 54.7 MB/s eta 0:00:03\n",
            "   ----------- ---------------------------- 55.8/198.6 MB 54.4 MB/s eta 0:00:03\n",
            "   ----------- ---------------------------- 58.3/198.6 MB 59.5 MB/s eta 0:00:03\n",
            "   ------------ --------------------------- 61.8/198.6 MB 59.5 MB/s eta 0:00:03\n",
            "   ------------- -------------------------- 65.1/198.6 MB 65.6 MB/s eta 0:00:03\n",
            "   ------------- -------------------------- 68.1/198.6 MB 65.6 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 69.9/198.6 MB 59.5 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 72.5/198.6 MB 59.5 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 75.5/198.6 MB 59.5 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 78.1/198.6 MB 54.4 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 80.7/198.6 MB 54.7 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 83.4/198.6 MB 54.7 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 86.0/198.6 MB 54.7 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 88.6/198.6 MB 54.4 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 90.6/198.6 MB 54.7 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 92.4/198.6 MB 50.4 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 95.5/198.6 MB 54.4 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 97.5/198.6 MB 54.4 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 99.5/198.6 MB 50.4 MB/s eta 0:00:02\n",
            "   ------------------- ------------------- 101.2/198.6 MB 46.7 MB/s eta 0:00:03\n",
            "   -------------------- ------------------ 103.1/198.6 MB 43.7 MB/s eta 0:00:03\n",
            "   -------------------- ------------------ 105.1/198.6 MB 40.9 MB/s eta 0:00:03\n",
            "   -------------------- ------------------ 105.6/198.6 MB 34.4 MB/s eta 0:00:03\n",
            "   --------------------- ----------------- 107.7/198.6 MB 34.4 MB/s eta 0:00:03\n",
            "   --------------------- ----------------- 110.4/198.6 MB 36.3 MB/s eta 0:00:03\n",
            "   ---------------------- ---------------- 113.0/198.6 MB 40.9 MB/s eta 0:00:03\n",
            "   ---------------------- ---------------- 115.7/198.6 MB 54.7 MB/s eta 0:00:02\n",
            "   ----------------------- --------------- 119.4/198.6 MB 65.6 MB/s eta 0:00:02\n",
            "   ------------------------ -------------- 122.6/198.6 MB 65.6 MB/s eta 0:00:02\n",
            "   ------------------------ -------------- 125.4/198.6 MB 65.6 MB/s eta 0:00:02\n",
            "   ------------------------- ------------- 128.5/198.6 MB 59.5 MB/s eta 0:00:02\n",
            "   ------------------------- ------------- 131.9/198.6 MB 59.5 MB/s eta 0:00:02\n",
            "   -------------------------- ------------ 134.6/198.6 MB 65.2 MB/s eta 0:00:01\n",
            "   -------------------------- ------------ 137.3/198.6 MB 65.6 MB/s eta 0:00:01\n",
            "   --------------------------- ----------- 140.4/198.6 MB 65.6 MB/s eta 0:00:01\n",
            "   ---------------------------- ---------- 143.8/198.6 MB 65.6 MB/s eta 0:00:01\n",
            "   ---------------------------- ---------- 146.7/198.6 MB 59.5 MB/s eta 0:00:01\n",
            "   ----------------------------- --------- 149.7/198.6 MB 59.5 MB/s eta 0:00:01\n",
            "   ----------------------------- --------- 152.6/198.6 MB 59.8 MB/s eta 0:00:01\n",
            "   ------------------------------ -------- 155.8/198.6 MB 65.6 MB/s eta 0:00:01\n",
            "   ------------------------------- ------- 158.6/198.6 MB 59.5 MB/s eta 0:00:01\n",
            "   ------------------------------- ------- 161.2/198.6 MB 59.5 MB/s eta 0:00:01\n",
            "   -------------------------------- ------ 164.1/198.6 MB 54.4 MB/s eta 0:00:01\n",
            "   -------------------------------- ------ 166.3/198.6 MB 59.5 MB/s eta 0:00:01\n",
            "   --------------------------------- ----- 169.4/198.6 MB 59.5 MB/s eta 0:00:01\n",
            "   --------------------------------- ----- 172.9/198.6 MB 65.2 MB/s eta 0:00:01\n",
            "   ---------------------------------- ---- 175.8/198.6 MB 65.6 MB/s eta 0:00:01\n",
            "   ----------------------------------- --- 178.9/198.6 MB 65.6 MB/s eta 0:00:01\n",
            "   ----------------------------------- --- 181.4/198.6 MB 54.4 MB/s eta 0:00:01\n",
            "   ------------------------------------ -- 184.5/198.6 MB 59.5 MB/s eta 0:00:01\n",
            "   ------------------------------------ -- 188.3/198.6 MB 65.2 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 190.5/198.6 MB 59.8 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 193.2/198.6 MB 65.6 MB/s eta 0:00:01\n",
            "   --------------------------------------  196.8/198.6 MB 59.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  198.6/198.6 MB 50.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  198.6/198.6 MB 50.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  198.6/198.6 MB 50.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  198.6/198.6 MB 50.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  198.6/198.6 MB 50.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  198.6/198.6 MB 50.4 MB/s eta 0:00:01\n",
            "   --------------------------------------- 198.6/198.6 MB 21.9 MB/s eta 0:00:00\n",
            "Downloading torchdata-0.7.1-cp311-cp311-win_amd64.whl (1.3 MB)\n",
            "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.3/1.3 MB 42.6 MB/s eta 0:00:00\n",
            "Installing collected packages: torch, torchdata, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.0\n",
            "    Uninstalling torch-2.2.0:\n",
            "      Successfully uninstalled torch-2.2.0\n",
            "Successfully installed torch-2.2.1 torchdata-0.7.1 torchtext-0.17.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.0 requires torch==2.2.0, but you have torch 2.2.1 which is incompatible.\n",
            "torchvision 0.17.0 requires torch==2.2.0, but you have torch 2.2.1 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "! pip install torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "mJWUqNi0QBJR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ".vector_cache\\glove.6B.zip: 862MB [02:40, 5.38MB/s]                               \n",
            "100%|█████████▉| 399999/400000 [00:16<00:00, 23641.53it/s]\n"
          ]
        }
      ],
      "source": [
        "import torchtext\n",
        "\n",
        "# The first time you run this will download a ~823MB file\n",
        "glove = torchtext.vocab.GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus\n",
        "                              dim=100)    # embedding size = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxQpDvu79_tN"
      },
      "source": [
        "Thankfully for us, GloVe embeddings are _incredibly_ easy to use. All you need to do to find the embedding for a word is to look it up in the embedding dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "UicSzTjMQTts"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.3905, -0.2549, -0.1382,  0.0420,  0.5667,  0.7832, -0.1071, -0.0431,\n",
            "         0.0406,  0.0567,  0.6232,  0.0259,  0.6145, -0.3968,  0.5271,  1.0976,\n",
            "         1.0748, -0.0540,  0.5088, -0.6904, -0.4768, -0.1957, -0.2951, -0.0348,\n",
            "         0.5413, -0.5468, -0.2390,  0.0017,  0.1913, -0.1182,  0.5662, -0.0543,\n",
            "         0.3056,  0.7888,  0.0410, -0.0877, -0.0585, -0.1933,  0.2155, -0.3460,\n",
            "        -0.2254,  0.0618,  0.1725,  0.4942, -0.3798,  0.1681,  0.0382,  0.0229,\n",
            "        -0.2144, -0.2600,  0.7728,  0.2907,  0.3781,  1.1977,  0.1553, -1.5200,\n",
            "        -0.5735,  0.0732,  0.6663, -0.5527, -0.1163,  1.6117,  0.0992, -0.2084,\n",
            "         0.6418,  0.2522,  0.8185,  0.5056, -0.2862,  0.4347,  0.0067,  0.3870,\n",
            "         0.4920,  0.1046,  0.3839,  0.6068,  0.5392,  0.0242, -0.3608, -0.3601,\n",
            "         0.1969, -0.1676,  0.7339, -0.2906, -0.2308, -1.0330,  0.5002, -0.2225,\n",
            "         0.2336, -0.1754,  0.2848, -0.4200,  1.1582, -0.6518, -0.2022, -0.3428,\n",
            "        -0.4344, -0.4060, -0.2982, -0.2774])\n",
            "torch.Size([100])\n"
          ]
        }
      ],
      "source": [
        "# Looking at an embedding for the word 'photograph'\n",
        "print(glove['photograph'])\n",
        "\n",
        "print(glove['photograph'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT6-dx9R-I1l"
      },
      "source": [
        "There are a number of ways to measure similarity between vectors (some of which can be found [here](https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity). One way of doing so is by calculating the _Euclidean Distance_, which is captures the difference between the ends of vectors. The larger the Euclidean Distance, the further away the vectors are in semantic space. To calculate Euclidean Distance, we take the vector norm of the two tensors, which essentially calculates the difference between the vectors at each level of embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "CI1oqF24Q9It"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EucDis between x, y: tensor(4.1111)\n",
            "EucDis between x, z: tensor(5.4529)\n"
          ]
        }
      ],
      "source": [
        "# Euclidean distance\n",
        "x = glove['photograph']\n",
        "y = glove['picture']\n",
        "z = glove['sculpture']\n",
        "print('EucDis between x, y:', torch.norm(y - x))\n",
        "print('EucDis between x, z:', torch.norm(z- x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36spGgH-K63t"
      },
      "source": [
        "We can also re-use our cosine similarity function from earlier to check out how similar the vectors are in semantic space:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "qTO_76K3RJuB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CoSim between x, y: tensor(0.6862)\n",
            "CoSim between x, z: tensor(0.4489)\n"
          ]
        }
      ],
      "source": [
        "# Cosine sim\n",
        "\n",
        "# Photograph & picture\n",
        "print('CoSim between x, y:',cs(x, y))\n",
        "\n",
        "# Photograph & sculpture\n",
        "print('CoSim between x, z:',cs(x, z))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c3DSXBgLM4g"
      },
      "source": [
        "Let's move on to another example. Given the following, how do you think the cosine similarities will vary?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "2zJY4Bh2RQLJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CoSim between cat and kitten: tensor(0.5581)\n"
          ]
        }
      ],
      "source": [
        "# Note the following:\n",
        "c = glove['cat']\n",
        "k = glove['kitten']\n",
        "d = glove['dog']\n",
        "\n",
        "# Cat & kitten\n",
        "print('CoSim between cat and kitten:', cs(c, k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "JG9fSLsUSaTf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CoSim between cat and dog: tensor(0.8798)\n"
          ]
        }
      ],
      "source": [
        "# Cat & dog\n",
        "print('CoSim between cat and dog:', cs(c, d))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCuHCKsQLhf6"
      },
      "source": [
        "Hold up! Let's _paws_ for a moment: why are cat & dog more similar than cat & kitten?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEe0bT4gLgtf"
      },
      "source": [
        "Let's try to pick apart which words are close to each other:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "ECiC0WMySawM"
      },
      "outputs": [],
      "source": [
        "# Function from this link: https://www.cs.toronto.edu/~lczhang/321/lec/glove_notes.html\n",
        "\n",
        "# This function finds which words are closest to one another via Euclidean Distance:\n",
        "def print_closest_words(vec, n=5):\n",
        "    dists = torch.norm(glove.vectors - vec, dim=1)                # Compute Euclidean Distances to all words\n",
        "    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1])    # Sort by distance\n",
        "    for idx, difference in lst[1:n+1]: \t\t\t\t\t                  # Take the n most related words\n",
        "        print(glove.itos[idx], difference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "IQvrFapFS6nk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dog 2.681131\n",
            "rabbit 3.6489706\n",
            "cats 3.6892004\n",
            "monkey 3.7469323\n",
            "puppy 3.9275599\n",
            "pet 3.949972\n",
            "dogs 4.0555873\n",
            "rat 4.131533\n",
            "mouse 4.1978264\n",
            "spider 4.2696805\n"
          ]
        }
      ],
      "source": [
        "# Looking at cats!\n",
        "print_closest_words(glove[\"cat\"], n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "dH5W8biSOkDu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "puppy 3.6014626\n",
            "tomboy 3.9557345\n",
            "moppet 4.018781\n",
            "pooch 4.0337315\n",
            "kittens 4.0392575\n",
            "yorkie 4.0828166\n",
            "pterodactyl 4.0928903\n",
            "tarantula 4.0984063\n",
            "dachshund 4.104253\n",
            "milkmaid 4.105781\n"
          ]
        }
      ],
      "source": [
        "# Looking at kitten\n",
        "print_closest_words(glove[\"kitten\"], n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYRB08WB8qZQ"
      },
      "source": [
        "Here, the only carry-over between \"cat\" and \"kitten\" is \"puppy\". This suggests that \"cat\" and \"kitten\" have very different co-occurence distributions in the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LztVHsJMROQ"
      },
      "source": [
        "We can also implement some of the \"vector math\" from previous lectures:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "FWnUX7JGS78k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "queen 4.081079\n",
            "monarch 4.6429076\n",
            "throne 4.9055004\n",
            "elizabeth 4.921559\n",
            "prince 4.981147\n"
          ]
        }
      ],
      "source": [
        "# The most famous word math example (include visualization)\n",
        "print_closest_words(glove['king'] - glove['man'] + glove['woman'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "Z9-x1z1gTKQe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "h3n2 6.088272\n",
            "h9n2 6.130206\n",
            "h9 6.1741743\n",
            "h-7 6.19326\n",
            "epstein-barr 6.2125893\n"
          ]
        }
      ],
      "source": [
        "# Any predictions about this one:\n",
        "print_closest_words(glove['avian'] - glove['fly'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvuntoKKOyDB"
      },
      "source": [
        "While embeddings are certainly fun tools to play with, it's important to remember that there are some implicit biases encoded into their structure; this is a consequence of the data the embeddings were trained on. Consider the following examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "nrsrszi3TP6-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "paramedic 4.602735\n",
            "schoolteacher 4.8263574\n",
            "nurse 5.001752\n",
            "homemaker 5.0627832\n",
            "janitor 5.10184\n"
          ]
        }
      ],
      "source": [
        "print_closest_words(glove['firefighter'] - glove['man'] + glove['woman'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "4t2BSyahVEWs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fireman 3.9453492\n",
            "mechanic 4.8609853\n",
            "ranger 4.8943105\n",
            "watchman 4.95111\n",
            "paramedic 4.986004\n"
          ]
        }
      ],
      "source": [
        "print_closest_words(glove['firefighter'] - glove['woman'] + glove['man'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlroB0ifaH1a"
      },
      "source": [
        "<a name=\"part4\"></a>\n",
        "[[^^^]](#outline)\n",
        "## **Part 4: Building a Feed Forward Neural Network (FFNN)**\n",
        "\n",
        "It is now time to learn how to construct your first neural model! This section is a Frankensteinien creation from a couple different PyTorch tutorials found on the [Pytorch website tutorials](https://pytorch.org/tutorials) and the [Udacity Deep Learning with Pytorch repository](https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-to-pytorch).\n",
        "\n",
        "To begin, let's state the task at hand: _identifying text in an image_. In order to start this task, we will need a dataset that provides us text in image form. The MNIST dataset consists of greyscale handwritten digits 0-9 (see image below). Each image of a digit is 28x28 pixels.\n",
        "\n",
        "![](https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/3a95d118f9df5a86826e1791c5c100817f0fd924/intro-to-pytorch/assets/mnist.png \"MNIST image examples\")\n",
        "\n",
        "We need to download this dataset from the `torchvision` package using the code below. This code will also create training and test datasets for us!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "IN0XoOP1aMmi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to C:\\Users\\bhave/.pytorch/MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 29123250.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting C:\\Users\\bhave/.pytorch/MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz to C:\\Users\\bhave/.pytorch/MNIST_data/MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to C:\\Users\\bhave/.pytorch/MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 9131978.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting C:\\Users\\bhave/.pytorch/MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz to C:\\Users\\bhave/.pytorch/MNIST_data/MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to C:\\Users\\bhave/.pytorch/MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 17396193.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting C:\\Users\\bhave/.pytorch/MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to C:\\Users\\bhave/.pytorch/MNIST_data/MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to C:\\Users\\bhave/.pytorch/MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 1665838.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting C:\\Users\\bhave/.pytorch/MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to C:\\Users\\bhave/.pytorch/MNIST_data/MNIST\\raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3AhFmM8RDV0"
      },
      "source": [
        "We will want to feed our images into the model one at a time, so we will use the iter() function to accomplish this goal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "guTaysFjbQDv"
      },
      "outputs": [],
      "source": [
        "# Getting the data ready\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDI1qdH9RTg6"
      },
      "source": [
        "We should also know what our data look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "aOQUbXpHRa07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "print(type(images))\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8m6QqFARh6n"
      },
      "source": [
        "So now we see that each image is of 28x28 pixels, and there are 64 images in a batch. The 1 indicates that it's of one color scale (black-white).\n",
        "\n",
        "Here's what one of the original images looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "jggYSjHpu3lD"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualizing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzesJ8J5v3xL"
      },
      "source": [
        "We are now ready to build our model! Reminder: the goal of our model is to identify a piece of text (in this case, a number from 0-9) from an image. To accomplish this goal, we will create for a simple feed forward neural network (FFNN) with the following characteristics:\n",
        "\n",
        "1. A Linear layer that takes in **784** inputs and outputs **256** hidden units\n",
        "2. A Linear layer that takes in **256** hidden units and outputs **64** hidden units\n",
        "3. A Linear layer that takes in **64** hidden units and outputs **10** output units\n",
        "3. A softmax layer that determines the optimal label for the image (aka a number from 0-9)\n",
        "4. A Rectified Linear Unit (ReLU) activation function\n",
        "\n",
        "Thankfully for us, the Pytorch module `nn` makes building neural networks quite simple. Many functions -- Rectified Linear Unit, Softmax, Linear layers, etc. -- are pre-built into the `nn` module. Let's see how this works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "GR2auPCyu5XF"
      },
      "outputs": [],
      "source": [
        "class ffnn(nn.Module):\n",
        "    def __init__(self, ninp, nhid1, nhid2, nhid3, nout):\n",
        "        super().__init__()\n",
        "\n",
        "        # Layers\n",
        "        # self.embedding = nn.Embedding(num_dict, num_dim) #  Note that there isn't an embedding layer here! Why?\n",
        "        self.hidden1 = nn.Linear(ninp, nhid1)              # One-hot to vector form, we already have a vector form so Linear is densifying\n",
        "        self.hidden2 = nn.Linear(nhid1, nhid2)\n",
        "        self.hidden3 = nn.Linear(nhid2, nhid3)\n",
        "        self.output = nn.Linear(nhid3, nout)\n",
        "\n",
        "        # Functions\n",
        "        self.relu = nn.ReLU()\n",
        "        self.lsmax = nn.LogSoftmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Passing through first\n",
        "        hid1 = self.hidden1(x)\n",
        "        hid1_relu = self.relu(hid1)                       # Note that the ReLU function doesn't \"learn\" anything. It's simply\n",
        "                                                          # the activation function.\n",
        "        # Passing through second\n",
        "        hid2 = self.hidden2(hid1_relu)\n",
        "        hid2_relu = self.relu(hid2)\n",
        "\n",
        "        # Passing through second\n",
        "        hid3 = self.hidden3(hid2_relu)\n",
        "        hid3_relu = self.relu(hid3)\n",
        "\n",
        "        # Getting output\n",
        "        out = self.output(hid3_relu)\n",
        "\n",
        "        return self.lsmax(out)    # Log Softmax!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFpmR0QTxGnl"
      },
      "source": [
        "**Note on activation functions:** Broadly, activation functions inform our current layer about which neurons can pass on information to the next layer. An important characteristic of these activation functions is that they are *non-linear*. As the name suggests, non-linear activation functions allow our models to make predictions beyond those of simple linear ones, thus helping our models find more intricate patterns within our data and construct more complex representations. This often greatly improves model performance.\n",
        "\n",
        "A number of non-linear activation functions exist (with PyTorch having many already [built-in](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity). Some common ones are [Tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh), [Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid), and [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU). Try switching out ReLU with one of these other two to see how your model's performance changes!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K439Ka3_wAB8"
      },
      "source": [
        "We have just defined the model architecture for your FFNN! Great job :)\n",
        "\n",
        "We will now initialize the model with the necessary dimensions for each layer (see above for dimensions), along with defining our loss criterion ([negative log-likelihood loss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)) and our optimizer ([stochastic gradient descent](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)). For our optimizer, we will also define a learning rate: this learning rate determines how quickly/slowly our model will update.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "oAcEVkzITZwe"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "model = ffnn(784, 256, 128, 64, 10)       # DECLARE THE MODEL\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.003)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8b5xym5YzBb"
      },
      "source": [
        "Looks great, doesn't it? How do you think the model would perform if we ran it right now?\n",
        "\n",
        "You got it: _very poorly_. We haven't passed it any training data yet! Let's change that. In the following code, we will pass all of the training data through the model 5 times (with each full pass through called an \"epoch\"). For each image, we'll:\n",
        "\n",
        "1. zero our gradients to remove any leftovers from the previous training run\n",
        "2. pass our data through the model\n",
        "3. calculate the loss\n",
        "4. perform backpropagation\n",
        "4. implement our optimizer to tweak our model\n",
        "\n",
        "That sounds like a _whole_ bunch of things to do, most of which also sound pretty complicated. Some good news: each of those steps is _**one**_ line of code!\n",
        "\n",
        "Finally, after each epoch, we'll output our loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "IgZwWlgyvGz8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss for epoch 0: 2.2255746103298946\n",
            "Training loss for epoch 1: 1.6128639542598968\n",
            "Training loss for epoch 2: 0.836603416403982\n",
            "Training loss for epoch 3: 0.554729700565084\n",
            "Training loss for epoch 4: 0.4438975309130988\n",
            "Training loss for epoch 5: 0.39500649089116785\n",
            "Training loss for epoch 6: 0.3659358172337892\n",
            "Training loss for epoch 7: 0.3449925704519632\n",
            "Training loss for epoch 8: 0.3279666204982475\n",
            "Training loss for epoch 9: 0.3129912879206796\n",
            "Training loss for epoch 10: 0.2987587365037851\n",
            "Training loss for epoch 11: 0.28683000922933827\n",
            "Training loss for epoch 12: 0.2745753614934904\n",
            "Training loss for epoch 13: 0.26323734180950153\n",
            "Training loss for epoch 14: 0.2524616152191086\n",
            "Training loss for epoch 15: 0.24238922965250162\n",
            "Training loss for epoch 16: 0.2322490211329989\n",
            "Training loss for epoch 17: 0.22328784190880846\n",
            "Training loss for epoch 18: 0.21451775006441545\n",
            "Training loss for epoch 19: 0.20620733357346388\n",
            "Training loss for epoch 20: 0.19858884716481923\n",
            "Training loss for epoch 21: 0.19090814323329341\n",
            "Training loss for epoch 22: 0.18413263484676765\n",
            "Training loss for epoch 23: 0.17772092645181647\n",
            "Training loss for epoch 24: 0.1717849587128043\n"
          ]
        }
      ],
      "source": [
        "epochs = 25\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        images = images.view(images.shape[0], -1)\n",
        "\n",
        "        ###TO DO: Training pass###\n",
        "        optimizer.zero_grad()             # First thing to do! Do NOT forget to do zero your gradients :)\n",
        "        output = model.forward(images)    # Passing data through our model\n",
        "        loss = criterion(output, labels)  # Calculating loss\n",
        "        loss.backward()                   # Backpropagation isn't so scary after all!\n",
        "        optimizer.step()                  # Performing SGD\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(f\"Training loss for epoch {e}: {running_loss/len(trainloader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgwEeWql1lG7"
      },
      "source": [
        "**Note on loss criterion:** There are [a number of loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) that we can use to calculate how well our model predicted the intended labels. Common loss functions include Negative Log Likelihood, [Mean Squared Error](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss), and [Cross Entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss).\n",
        "\n",
        "In the code above, we calculate the loss by weighing our model's predictions against the actual labels in `loss = criterion(output, labels)`. This loss is a tensor with all the necessary modifications to apply to our model. Afterward, we apply `loss.backward()` to calculate the gradients of these modifications, which we then apply to our model using the `optimizer.step()` line. Note that our `optimizer` takes in our model's parameters, meaning the `optimizer` knows where to make changes to the model according to the instructions from the `loss.backward()` step.\n",
        "\n",
        "**Note on optimizer:** After each pass through the data, our optimizer will store all of the gradients. Since we do not want the information from the previous pass of data to additively affect our current model, we **must* zero our gradients. Please do not forget this step!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O07mSh03b64"
      },
      "source": [
        "**Note on why the model isn't perfect after one epoch:** In class, a few students asked why our model doesn't just \"learn everything in one pass\". There are many different reasons, but I will outline two key ones here:\n",
        "\n",
        "First, the data is noisy. My \"2\" is not the same as your \"2\", which is different than both Marty's and Lillian's \"2\"s. It is also extremely likely that each \"2: I write is (at least) minorly different in appearance. However, our brains have identified the general, abstract image of what a \"2\" looks like -- \"half a heart, with a line at the bottom going to the right\" (or something like that) -- but such a conceptual representation can be realized as an image in infinite different ways. As such, our model is trying to \"learn\" this abstract form from the data, meaning it needs to discover some complex representation of pixels that identifies a \"2\" from an \"8\". With each pass through the data, the model becomes better and better at identifying the complex patterns that lead it to the abstract \"2\" class.\n",
        "\n",
        "Second, the learning rate (here, lr = 0.003) determines how much the model can update at one point in time; learning rates are often between 0.01 and 0.001. The value of this learning rate matters quite a bit. If the learning rate is too high, the model will drastically change its parameters with each pass of the data, reducing the likelihood of model convergence because it cannot maintain its predictions. If the learning rate is too low, the model will make changes to its parameters that are too tiny, also reducing the likelihood that your model will converge, but for a different reason -- it may not reach the ideal local minima."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsmqKU_5aw2-"
      },
      "source": [
        "Note that our training loss goes down after each epoch -- this is good news! It means our model is becoming better at identifying the intended labels from the images it's receiving.\n",
        "\n",
        "\n",
        "For now, let's say our model has been sufficiently trained (though feel free to mess with the architecture/training process to see how you can improve performance). It is now time to check our model's predictions on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0HyZJGDd03K"
      },
      "source": [
        "The following three functions are for visualization purposes and do not have any effect on your model's training or testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "m6dRNYgbTu7O"
      },
      "outputs": [],
      "source": [
        "# Visualization function\n",
        "def imshow(image, ax=None, title=None, normalize=True):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "\n",
        "    if normalize:\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "\n",
        "    ax.imshow(image)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.tick_params(axis='both', length=0)\n",
        "    ax.set_xticklabels('')\n",
        "    ax.set_yticklabels('')\n",
        "\n",
        "    return ax\n",
        "\n",
        "# Visualization function\n",
        "def view_recon(img, recon):\n",
        "    ''' Function for displaying an image (as a PyTorch Tensor) and its\n",
        "        reconstruction also a PyTorch Tensor\n",
        "    '''\n",
        "\n",
        "    fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
        "    axes[0].imshow(img.numpy().squeeze())\n",
        "    axes[1].imshow(recon.data.numpy().squeeze())\n",
        "    for ax in axes:\n",
        "        ax.axis('off')\n",
        "        ax.set_adjustable('box-forced')\n",
        "\n",
        "# Visualization function\n",
        "def view_classify(img, ps, version=\"MNIST\"):\n",
        "    ''' Function for viewing an image and it's predicted classes.\n",
        "    '''\n",
        "    ps = ps.data.numpy().squeeze()\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
        "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
        "    ax1.axis('off')\n",
        "    ax2.barh(np.arange(10), ps)\n",
        "    ax2.set_aspect(0.1)\n",
        "    ax2.set_yticks(np.arange(10))\n",
        "    if version == \"MNIST\":\n",
        "        ax2.set_yticklabels(np.arange(10))\n",
        "    elif version == \"Fashion\":\n",
        "        ax2.set_yticklabels(['T-shirt/top',\n",
        "                            'Trouser',\n",
        "                            'Pullover',\n",
        "                            'Dress',\n",
        "                            'Coat',\n",
        "                            'Sandal',\n",
        "                            'Shirt',\n",
        "                            'Sneaker',\n",
        "                            'Bag',\n",
        "                            'Ankle Boot'], size='small');\n",
        "    ax2.set_title('Class Probability')\n",
        "    ax2.set_xlim(0, 1.1)\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNV8hMeE32rH"
      },
      "source": [
        "Let's see what we've got!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "cxOOjOW8vNyO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\bhave\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x900 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Loading test data\n",
        "images, labels = next(iter(testloader))\n",
        "\n",
        "img = images[0].view(1, 784)\n",
        "# Turn off gradients to speed up this part\n",
        "with torch.no_grad():\n",
        "    logps = model(img)\n",
        "\n",
        "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
        "ps = torch.exp(logps)\n",
        "view_classify(img.view(1, 28, 28), ps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW6NzS9KeOJy"
      },
      "source": [
        "Congratulations!!! You've trained a model to identify text from an image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jc1Pw7H1wDJ"
      },
      "source": [
        "**Note on why the model isn't perfect right away:** In class, a few students asked why our model doesn't just \"learn everything in one pass\". There are many different reasons, but I will outline two key ones here:\n",
        "\n",
        "First, the data is noisy. My \"2\" is not the same as your \"2\", which is different than both Marty's and Lillian's \"2\"s. It is also extremely likely that each \"2: I write is (at least) minorly different in appearance. However, our brains have identified the general, abstract image of what a \"2\" looks like -- \"half a heart, with a line at the bottom going to the right\" (or something like that) -- but such a conceptual representation can be realized as an image in infinite different ways. As such, our model is trying to \"learn\" this abstract form from the data, meaning it needs to discover some complex representation of pixels that identifies a \"2\" from an \"8\". With each pass through the data, the model becomes better and better at identifying the complex patterns that lead it to the abstract \"2\" class.\n",
        "\n",
        "Second, we are controlling how much the model can update at once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8A_Kjex3sfy"
      },
      "source": [
        "To perform additional test analyses (model perplexity, etc.), you can pass the entire test data through the model without updating any parameters or performing backpropagation and keep track of the total loss. However, this is a different task for another time... (cue mysterious music...).\n",
        "\n",
        "Anyway, that's it for this tutorial! Hope it helps :)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
