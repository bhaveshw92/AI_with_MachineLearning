{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4157c10",
   "metadata": {},
   "source": [
    "# Using SageMaker  'script mode' \n",
    "\n",
    "In this notebook we first start with loading scikit-learn inside this nodebook and I show you different parts of that. Then we use that knowldge to test the code make sure the local training jobs works. That means, in this jupyter instance, we load the sklearn and put some codes inside this notebook and we make the training job and model. \n",
    "\n",
    "If everything works fine, we use the code of this notebook as a refrence to create a script file (script.py that you also need to upload it to Jupyter file system) and then we are going to assume the SageMaker SKlearn container hosts our code. So we load that SageMaker SKLearn container and we pass our script file to it later.\n",
    "\n",
    "The script.py is essentially a converted version of the cells of this notebook, before we get into SageMaker part. We make a function for each cell in that script and we pass the parameters from this notebook to make sure that file can hook itself to SageMaker and runs inside SageMaker built-in containers (in this case sklearn container).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8724b87b",
   "metadata": {},
   "source": [
    "### Create  dummy dataset and save them into disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e006eb0b",
   "metadata": {},
   "source": [
    "Although we could get a data file from different data sources, my focus here is to show you how the SageMaker script mode works. So I use the \"datasets\" function inside sklearn to generate some dummy data for me.\n",
    "\n",
    "We also use pickle library to serialize and deserialize the data when we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e1b2241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6709b293",
   "metadata": {},
   "source": [
    "We use the \"make_regression\" function to create 100 data points with some noise and bias to provide some randomness into the generated data.\n",
    "\n",
    "The following line generates a synthetic dataset using the make_regression function from the datasets module in the scikit-learn library.\n",
    "\n",
    "The make_regression function generates a random regression problem, where the input features X are sampled from a **multivariate normal distribution** and the target variable y is a linear combination of the input features with some added noise.\n",
    "\n",
    "The arguments to the make_regression function are as follows:\n",
    "\n",
    "**100**: This specifies the number of samples to generate. In this case, we are generating a dataset with 100 samples.\n",
    "\n",
    "**1**: This specifies the number of input features to generate for each sample. In this case, we are generating a dataset with one input feature.\n",
    "\n",
    "**noise=5**: This specifies the standard deviation of the Gaussian noise added to the target variable y. In this case, we are adding a noise with a standard deviation of 5 to the target variable.\n",
    "\n",
    "**bias=0**: This specifies the intercept term in the linear model used to generate the target variable y. In this case, we are setting the intercept term to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d83b2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_regression(100, 1, noise=5, bias=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90b57ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.1154211 ],\n",
       "        [-0.53797108],\n",
       "        [ 1.28411277],\n",
       "        [ 0.41022989],\n",
       "        [-0.71581683],\n",
       "        [-0.11103686],\n",
       "        [ 0.21688263],\n",
       "        [-0.96370828],\n",
       "        [ 1.25859056],\n",
       "        [-0.92132718],\n",
       "        [-0.00442608],\n",
       "        [-0.45492271],\n",
       "        [ 0.35162384],\n",
       "        [ 0.71364796],\n",
       "        [ 0.34503081],\n",
       "        [ 1.59984851],\n",
       "        [-0.0839448 ],\n",
       "        [ 0.31472825],\n",
       "        [ 0.95771149],\n",
       "        [ 0.3927157 ],\n",
       "        [ 0.09097232],\n",
       "        [ 0.9225186 ],\n",
       "        [-0.40496473],\n",
       "        [-1.29520659],\n",
       "        [-0.34307335],\n",
       "        [ 0.61456245],\n",
       "        [ 0.43930244],\n",
       "        [ 0.55882438],\n",
       "        [ 0.82420809],\n",
       "        [ 0.87844747],\n",
       "        [ 0.20925097],\n",
       "        [ 0.93236776],\n",
       "        [ 0.87548411],\n",
       "        [-0.42784255],\n",
       "        [-1.34047366],\n",
       "        [ 1.17498392],\n",
       "        [-1.42331904],\n",
       "        [-0.02370424],\n",
       "        [-0.36027973],\n",
       "        [-0.23379689],\n",
       "        [-0.56051278],\n",
       "        [-0.15642898],\n",
       "        [-0.00813131],\n",
       "        [ 0.36243484],\n",
       "        [ 1.79982321],\n",
       "        [ 0.28020015],\n",
       "        [ 0.68837657],\n",
       "        [ 0.68577785],\n",
       "        [-0.89856442],\n",
       "        [ 0.19023464],\n",
       "        [ 1.60335093],\n",
       "        [ 0.2864042 ],\n",
       "        [ 1.23532865],\n",
       "        [-0.90610178],\n",
       "        [-0.52238212],\n",
       "        [ 0.44973409],\n",
       "        [ 0.22564513],\n",
       "        [ 1.48237443],\n",
       "        [ 0.28057659],\n",
       "        [ 1.21903685],\n",
       "        [ 0.13332183],\n",
       "        [-1.29701933],\n",
       "        [-1.35305979],\n",
       "        [-0.20471602],\n",
       "        [-0.76243325],\n",
       "        [-2.69227437],\n",
       "        [-0.17698922],\n",
       "        [ 1.02671596],\n",
       "        [-0.56354607],\n",
       "        [-0.41518697],\n",
       "        [ 0.62734298],\n",
       "        [-0.90382777],\n",
       "        [ 0.15381698],\n",
       "        [-0.59893859],\n",
       "        [-0.45412582],\n",
       "        [ 1.2815327 ],\n",
       "        [ 2.41787933],\n",
       "        [-0.69599338],\n",
       "        [ 0.51471279],\n",
       "        [ 0.64427053],\n",
       "        [ 0.88313336],\n",
       "        [ 0.23583631],\n",
       "        [-1.09000201],\n",
       "        [-0.12887137],\n",
       "        [ 1.05509456],\n",
       "        [ 0.25639217],\n",
       "        [-0.37223587],\n",
       "        [ 0.51347612],\n",
       "        [ 0.02425604],\n",
       "        [-0.56653204],\n",
       "        [ 1.48384683],\n",
       "        [-0.03446614],\n",
       "        [ 1.25842291],\n",
       "        [-0.98309705],\n",
       "        [-1.00373226],\n",
       "        [ 0.18932493],\n",
       "        [ 0.87913605],\n",
       "        [-0.86229092],\n",
       "        [ 1.57568621],\n",
       "        [-0.23364807]]),\n",
       " array([ 1.79487144e+00, -1.54213482e+01,  4.21060525e+01,  1.40829337e+01,\n",
       "        -1.76161027e+01, -8.32817099e+00,  2.09945794e+00, -2.36339133e+01,\n",
       "         3.50850639e+01, -1.96293205e+01, -1.98024315e-02, -8.40455581e+00,\n",
       "         7.68140541e+00,  1.44777938e+01,  7.83306169e+00,  3.58685380e+01,\n",
       "        -7.59237436e+00,  4.26511766e+00,  2.73145070e+01,  5.72529976e+00,\n",
       "         3.07570989e+00,  1.75577571e+01, -3.21816221e+00, -3.99489578e+01,\n",
       "        -1.53866969e+01,  2.38801334e+01,  8.77029212e+00,  1.37151974e+01,\n",
       "         1.75233494e+01,  2.81718618e+01,  2.11478908e+00,  1.99884161e+01,\n",
       "         3.12174231e+01, -7.32647112e+00, -3.64796485e+01,  3.44655571e+01,\n",
       "        -4.63690888e+01, -5.78080792e+00, -6.11827619e+00, -4.00123306e+00,\n",
       "        -1.69502998e+01,  1.71635830e+00, -3.60683052e+00,  9.52770452e+00,\n",
       "         5.17684100e+01,  1.06296289e+01,  1.64101317e+01,  1.31380245e+01,\n",
       "        -1.99177200e+01,  3.99051207e+00,  4.69004112e+01,  1.43686879e+01,\n",
       "         2.00449097e+01, -2.49772609e+01, -1.78363859e+01, -7.82910732e-01,\n",
       "         9.42980990e+00,  3.62270390e+01,  1.19650467e+01,  3.07422194e+01,\n",
       "        -4.75912226e-01, -3.59804960e+01, -3.56506505e+01, -7.32686385e+00,\n",
       "        -2.63946612e+01, -7.00213182e+01, -6.35019365e+00,  2.68639339e+01,\n",
       "        -1.40829261e+01, -5.99933996e+00,  1.67134697e+01, -2.56229160e+01,\n",
       "         3.92712171e+00, -1.33083804e+01, -5.13803567e+00,  4.94834427e+01,\n",
       "         6.54928189e+01, -2.19542278e+01,  1.71065780e+01,  1.80245184e+01,\n",
       "         1.57253230e+01,  9.08824784e+00, -3.21730970e+01, -8.45991410e-01,\n",
       "         1.94081415e+01,  8.69240848e+00, -1.26700106e+01,  1.20046650e+01,\n",
       "         4.31172442e+00, -1.64500643e+01,  3.78004531e+01, -5.84508397e+00,\n",
       "         2.92970546e+01, -3.27901363e+01, -3.23529331e+01, -9.22073229e+00,\n",
       "         2.47472462e+01, -3.08998042e+01,  3.93336346e+01, -1.34922034e+01]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5fc232",
   "metadata": {},
   "source": [
    "Then we save those data points into a file called \"traing.pickle\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c25527fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([X,y], open('./train.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a601e752",
   "metadata": {},
   "source": [
    "## Create a model from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f3777e",
   "metadata": {},
   "source": [
    "In the next step, we load the LinearRegression algorithm from the sklearn and we pass the generated data in the previous section to this algorithm to create a model of our generated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aa04224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69715869",
   "metadata": {},
   "source": [
    "We load the data that we serialize into disk in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00caf709",
   "metadata": {},
   "outputs": [],
   "source": [
    "[loaded_X, loaded_y] = pickle.load(open('./train.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24035b21",
   "metadata": {},
   "source": [
    "We create an instance of algorithm and call it \"model\" object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc9f9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47ba5aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(loaded_X,loaded_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0ba432",
   "metadata": {},
   "source": [
    "## Test the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee84f1b",
   "metadata": {},
   "source": [
    "We have a model now. We want to test if it works or not. For that we pass an array of input and see if it predicts some outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee21fc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.89607839, 26.02072813, 52.93753466, 79.85434118])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[0],[1],[2],[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd30d6a",
   "metadata": {},
   "source": [
    "So that works. Now the next step is we save this model file into disk and load it again to make sure it works even after loading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7df6e",
   "metadata": {},
   "source": [
    "## Save the model to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282ee15",
   "metadata": {},
   "source": [
    "We use the same serialization library we used to save and load the data but this time we use it for model file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb971804",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pickle.dumps(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe2d9f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('./model.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaa7fed",
   "metadata": {},
   "source": [
    "## Loading the model from a file and test it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94e63048",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('./model.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65309fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.89607839, 26.02072813, 52.93753466, 79.85434118])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict([[0],[1],[2],[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76dfd2",
   "metadata": {},
   "source": [
    "# SageMaker Training in script mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182bf29e",
   "metadata": {},
   "source": [
    "From this cell and on, the SageMaker part starts. The Script.py is going to have functions for the above mentioned cells. \n",
    "\n",
    "Now that we have a code that upload data and model and creates a model from the data, we want to pass that to SageMaker and create that model by SageMaker. Amazon SageMaker Python SDK consider all those well-know ML libraries like sklearn, pytorch, Tensorflow and MXnet as first level citizens and allows you to use them inside the SagaMaker. You just need to point SageMaker Python SDK that you want to use them and the rest is passing the arguments and parameters to that container that will run your code.\n",
    "\n",
    "In the following code, we point SageMaker library to sklearn library and import other required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ff80053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc493735",
   "metadata": {},
   "source": [
    "Since we are going to use SageMaker, We need to set the role that the training instance will use, the location of the data files and the model artifact in S3. In the following, we create  some variables that will be used through this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39b1aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "s3_prefix = \"script-mode\"\n",
    "data_s3_prefix = f\"{s3_prefix}/training_data\"\n",
    "data_train_s3_uri = f\"s3://{bucket}/{s3_prefix}/training_data\"\n",
    "\n",
    "train_dir = os.path.join(os.getcwd(), \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc58d3ed",
   "metadata": {},
   "source": [
    "Let's see the values of the above variables before we use them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cb62276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::566462208046:role/LabRole'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "674038ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'script-mode'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "458d9efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'script-mode/training_data'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_s3_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7af75da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-566462208046/script-mode/training_data'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5901134",
   "metadata": {},
   "source": [
    "The following access is needed to run the script sucessfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d2aec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo chmod 777 lost+found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfe3382",
   "metadata": {},
   "source": [
    "Upload the training data to S3, so it's available for SageMaker training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63900fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource_bucket = boto3.Session().resource(\"s3\").Bucket(bucket)\n",
    "s3_resource_bucket.Object(os.path.join(data_s3_prefix, \"train.pickle\")).upload_file(\n",
    "    train_dir + \"/train.pickle\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c6af96",
   "metadata": {},
   "source": [
    "After running the above line the data file is loaded into script-mode/training_data/ folder in the SageMaker folder. So the data now is on S3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa276651",
   "metadata": {},
   "source": [
    "In the code I showed you earlier, before getting into SageMaker part, I did not set any hyperparameter and just used the default values. This time, I am going to show you how to set those up, if you want to, and how these hyperparameters are passed to SageMaker container later. To undrestand how these hyperparameters are related to the script we create later, see the follwowing three lines and compare them with the next cell:\n",
    "\n",
    "    parser.add_argument(\"--copy_X\",        type=bool, default=True)\n",
    "    parser.add_argument(\"--fit_intercept\", type=bool, default=True)\n",
    "    parser.add_argument(\"--normalize\",     type=bool, default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08b12c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"copy_X\": True,\n",
    "    \"fit_intercept\": True,\n",
    "    \"normalize\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8139b1",
   "metadata": {},
   "source": [
    "Then we set a few more variables that we need to pass to the SageMaker Estimator later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f3444ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type = \"ml.m5.large\"\n",
    "\n",
    "inputs = {\n",
    "    \"train\": data_train_s3_uri\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75f39d3",
   "metadata": {},
   "source": [
    "Now we create the The SageMaker Estimator object. That is a high level interface for SageMaker training.  This object represents the algorithm, the data, and other configuration. \n",
    "\n",
    "https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html\n",
    "\n",
    "The most important parts of the following code are:\n",
    "\n",
    "1) I have an entry_point that is set to script.py. Undrestanding that code is crucial. You need to open that file and relate that to whatever has happened in this notebook. \n",
    "2) We set where that script is. Here we set the current directory (.)\n",
    "3) We also have to set the sklearn and python versions\n",
    "\n",
    "The rest are like other SageMaker Estimator settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3989d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_parameters = {\n",
    "    \"entry_point\": \"script.py\",\n",
    "    \"source_dir\": \".\",\n",
    "    \"framework_version\": \"0.23-1\",\n",
    "    \"py_version\": \"py3\",\n",
    "    \"instance_type\": train_instance_type,\n",
    "    \"instance_count\": 1,\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"role\": role,\n",
    "    \"base_job_name\": \"linearregression-model\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181abcf0",
   "metadata": {},
   "source": [
    "Now we create an instance of the SageMaker Sklearn object and we pass the above mentioned parameters to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f02af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SKLearn(**estimator_parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d5035f",
   "metadata": {},
   "source": [
    "Then we use .fit() function to let SageMaker to spin up a managed instance, transfer the code (script.py) and data and put that into a sklearn container to start the training.  All this happens off of the notebook server.  We can watch the training through the console, and watch the logs in CloudWatch Logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1869306e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: linearregression-model-2023-02-26-03-10-21-266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-26 03:10:22 Starting - Starting the training job...\n",
      "2023-02-26 03:10:38 Starting - Preparing the instances for training......\n",
      "2023-02-26 03:11:24 Downloading - Downloading input data...\n",
      "2023-02-26 03:11:54 Training - Downloading the training image..\u001b[34m2023-02-26 03:12:34,719 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2023-02-26 03:12:34,722 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-02-26 03:12:34,761 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-02-26 03:12:34,948 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-02-26 03:12:34,960 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-02-26 03:12:34,971 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-02-26 03:12:34,981 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"copy_X\": true,\n",
      "        \"fit_intercept\": true,\n",
      "        \"normalize\": false\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"linearregression-model-2023-02-26-03-10-21-266\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-566462208046/linearregression-model-2023-02-26-03-10-21-266/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"script\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"script.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"copy_X\":true,\"fit_intercept\":true,\"normalize\":false}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=script.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=script\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-566462208046/linearregression-model-2023-02-26-03-10-21-266/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"copy_X\":true,\"fit_intercept\":true,\"normalize\":false},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"linearregression-model-2023-02-26-03-10-21-266\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-566462208046/linearregression-model-2023-02-26-03-10-21-266/source/sourcedir.tar.gz\",\"module_name\":\"script\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"script.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--copy_X\",\"True\",\"--fit_intercept\",\"True\",\"--normalize\",\"False\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_COPY_X=true\u001b[0m\n",
      "\u001b[34mSM_HP_FIT_INTERCEPT=true\u001b[0m\n",
      "\u001b[34mSM_HP_NORMALIZE=false\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python script.py --copy_X True --fit_intercept True --normalize False\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34mTraining...\u001b[0m\n",
      "\u001b[34m2023-02-26 03:12:35,699 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-02-26 03:12:55 Uploading - Uploading generated training model\n",
      "2023-02-26 03:12:55 Completed - Training job completed\n",
      "Training seconds: 92\n",
      "Billable seconds: 92\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a10fb",
   "metadata": {},
   "source": [
    "In the above output pay attention to where the model file is stored in the S3 bucket. It would be something like thuis:\n",
    "\n",
    "SM_MODULE_DIR=s3://sagemaker-us-east-1-722464288670/linearregression-model-2022-11-18-00-07-00-294/source/sourcedir.tar.gz\n",
    "\n",
    "This is where the model.tar.gz is located on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411616f",
   "metadata": {},
   "source": [
    "# (Optional) SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34a55be",
   "metadata": {},
   "source": [
    "Since we have not talked about the inferencing by the SageMaker this part is optional but in the case you want to see what you can do after, please continue reading this notebook. \n",
    "\n",
    "After running the .fit(), a new model artifact is created in the current folder .\n",
    "\n",
    "We can now take create a 'predictor' by deploying the estimator.  Then we can use it to make new predictions.\n",
    "\n",
    "(Make sure that the 'endpoint_name' used is not currently running.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533cac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: linearregression-model-2023-02-26-03-13-06-081\n",
      "INFO:sagemaker:Creating endpoint-config with name linearregression-endpoint1\n",
      "INFO:sagemaker:Creating endpoint with name linearregression-endpoint1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------"
     ]
    }
   ],
   "source": [
    "sklearn_predictor = estimator.deploy(initial_instance_count=1,\n",
    "                                     instance_type='ml.m5.large',\n",
    "                                     endpoint_name='linearregression-endpoint1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e637516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_predictor.predict([[0],[1],[2],[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0a73ee",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79fc5a4",
   "metadata": {},
   "source": [
    "Running this cell will remove the endpoint and configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7132c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_predictor.delete_endpoint(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebde47f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea381bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
