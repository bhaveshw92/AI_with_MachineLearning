{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning word embeddings with `Gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we are going to perform word embeddings with Gensim - a very popular toolkit for Natural Language Processing (NLP). We will build two word embeddings: `word2vec` and `fastText`.\n",
    "\n",
    "First, let's start to import some useful packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bhave\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import pandas, numpy, tensorflow, nltk, re, keras,datetime, gensim\n",
    "\n",
    "import logging\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Dataset: \"Bag of Words Meets Bags of Popcorn\". We will work with not only the training set but also the unlabled training set. Because we only want to learn word embedding (not building model) so we don't need labels in the set, making the unlabeled data useful.\n",
    "\n",
    "Now, let's load them using `pandas` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries for execution.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import word2vec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid', context='talk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into dataframe the labeledTrainData and unlabeledTrainData\n",
    "\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", delimiter=\"\\t\", header = 0, quoting=3)\n",
    "unlabeled = pd.read_csv(\"unlabeledTrainData.tsv\", delimiter=\"\\t\", header = 0, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 25,000 reviews in the training set and 50,000 reviews in the unlabeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (25000, 3)\n",
      "Unlabeled set: (50000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the dataset provided\n",
    "\n",
    "print(\"Train set: {}\".format(train.shape))\n",
    "print(\"Unlabeled set: {}\".format(unlabeled.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick review about the first unlabeled review. We can see that thare are many HTML tags in this review. So our task is to clean up those reviews by removing the unwanted characters such as HTML tags and special characters `(\\, /, ?, !, etc.)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"I saw this film about 20 years ago and remember it as being particularly nasty. I believe it is based on a true incident: a young man breaks into a nurses\\' home and rapes, tortures and kills various women.<br /><br />It is in black and white but saves the colour for one shocking shot.<br /><br />At the end the film seems to be trying to make some political statement but it just comes across as confused and obscene.<br /><br />Avoid.\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled.review[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function <span style=\"color:blue; font-family:Courier\">convert_to_sentences</span> is defined in order to remove special characters. Steps are as below:\n",
    "1. Convert each review into sentences as we might have many sentences in one review by using `punkt` tokenizer from NLTK\n",
    "2. Replace HTML tags with space. Character '-' with space also, and add space after '.' just to help tokenizer to strip sentence correctly. If we don't do this, sentences connected with '.' (no space after) will be considered as one sentence which is not correct. And replace double space with one space only.\n",
    "3. Convert each sentence into words and apply the `special_characters` filter on it. Then convert all of the into lowercase.\n",
    "4. Finally, return a list of reviews which contain inside each review a list of sentences made up a list of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews are been preprocessed in this function all the html tags and all the special characters are been identified and been replaces with space for better processing the sentences.\n",
    "\n",
    "special_characters = re.compile(\"[^A-Za-z0-9 ]\")\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "def convert_to_sentences(data, tokenizer):\n",
    "    # First, converting each review into sentences\n",
    "    # Use NLTK Tokenizer to split review into sentences (punkt tokenizer - english.pickle)\n",
    "    data = data.lower().replace(\"<br />\", \" \")\n",
    "    data = data.replace(\"-\", \" \")\n",
    "    data = data.replace(\".\", \". \")\n",
    "    data = re.sub(\"  \", \" \", data)\n",
    "    all_sentences = tokenizer.tokenize(data.strip())\n",
    "    \n",
    "    # Second, converting each sentence into words\n",
    "    sentences = []\n",
    "    for words in all_sentences:\n",
    "        s = re.sub(special_characters, \"\", words.lower())\n",
    "        if (len(s)) > 0:\n",
    "            sentences.append(s.split())\n",
    "    \n",
    "    # Finally, returning a list of sentences (containing words in each sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a sample of raw review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: \n",
      "\"Steven Seagal returns in Black Dawn.Cool title.I cannot say the same for the movie though.It ranks as the Worst Seagal movie yet.Yes.It is worse than Out for a Kill.Did we really need a sequel to the Forigner that bad?<br /><br />Steven returns as Jonathon Cold.Assassin for hire.This time he is out to stop a group of Terrorists and prevent Nuclier Bomb from hitting Los Angelas with the help of CIA agent Tamara Davies.<br /><br />This idea seems okay.The Jonathon Cold character once again gives the big guy a chance to tarnish his clean-cut cinematic Image.You get the feeling with a decent director and production team this movie could have went somewhere.If this cost 15 million I wanna know where it was spent?It was not spent on the movie?!! <br /><br />The action scenes are not bad.The movie was obviously shot like a Doug Liman flick.Its just that most of the Time Seagal is doubled so much you cannot take the movie seriously whatsoever.The script seems...what script?!Logic is Black Dawned for a dozen low-budget action movies.<br /><br />Seagal is not too bad here for a whole 15 minutes he is engaging then goes about the sleepwalking that he has done for his last 2 movies.Whoever is doing his fighting is doing a commendable job.<br /><br />The light that shines through is Tamara Davies as Cold's sidekick.She has the chops,skills,looks and drive that give Black Dawn a little kick in the action department.How she got in this movie is a riddle.She should be a bigger star.Nicholas Davidoff and John Pyper Feruson are not believable bad guys.<br /><br />Whose idea was it to make a sequel to a bad movie in the first place? It did not set the world on fire to my memory.I can only hope they inject the next Forigner sequel with some class,imagination,Less stunt doubles and a livelier Seagal.If not then leave Jonathon Cold on Ice.\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Review: \\n{}\".format(unlabeled.review[90]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the function <span style=\"color:blue; font-family:Courier\">convert_to_sentences</span> to this review, we get 31 sentences in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of formated sentences: 31\n"
     ]
    }
   ],
   "source": [
    "# Check the results from the function application and review them.\n",
    "\n",
    "sentences_sample = convert_to_sentences(unlabeled.review[90], tokenizer)\n",
    "print(\"Length of formated sentences: {}\".format(len(sentences_sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details of each sentence are as followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['steven', 'seagal', 'returns', 'in', 'black', 'dawn']\n",
      "['cool', 'title']\n",
      "['i', 'cannot', 'say', 'the', 'same', 'for', 'the', 'movie', 'though']\n",
      "['it', 'ranks', 'as', 'the', 'worst', 'seagal', 'movie', 'yet']\n",
      "['yes']\n",
      "['it', 'is', 'worse', 'than', 'out', 'for', 'a', 'kill']\n",
      "['did', 'we', 'really', 'need', 'a', 'sequel', 'to', 'the', 'forigner', 'that', 'bad']\n",
      "['steven', 'returns', 'as', 'jonathon', 'cold']\n",
      "['assassin', 'for', 'hire']\n",
      "['this', 'time', 'he', 'is', 'out', 'to', 'stop', 'a', 'group', 'of', 'terrorists', 'and', 'prevent', 'nuclier', 'bomb', 'from', 'hitting', 'los', 'angelas', 'with', 'the', 'help', 'of', 'cia', 'agent', 'tamara', 'davies']\n",
      "['this', 'idea', 'seems', 'okay']\n",
      "['the', 'jonathon', 'cold', 'character', 'once', 'again', 'gives', 'the', 'big', 'guy', 'a', 'chance', 'to', 'tarnish', 'his', 'clean', 'cut', 'cinematic', 'image']\n",
      "['you', 'get', 'the', 'feeling', 'with', 'a', 'decent', 'director', 'and', 'production', 'team', 'this', 'movie', 'could', 'have', 'went', 'somewhere']\n",
      "['if', 'this', 'cost', '15', 'million', 'i', 'wanna', 'know', 'where', 'it', 'was', 'spentit', 'was', 'not', 'spent', 'on', 'the', 'movie']\n",
      "['the', 'action', 'scenes', 'are', 'not', 'bad']\n",
      "['the', 'movie', 'was', 'obviously', 'shot', 'like', 'a', 'doug', 'liman', 'flick']\n",
      "['its', 'just', 'that', 'most', 'of', 'the', 'time', 'seagal', 'is', 'doubled', 'so', 'much', 'you', 'cannot', 'take', 'the', 'movie', 'seriously', 'whatsoever']\n",
      "['the', 'script', 'seems']\n",
      "['what', 'script']\n",
      "['logic', 'is', 'black', 'dawned', 'for', 'a', 'dozen', 'low', 'budget', 'action', 'movies']\n",
      "['seagal', 'is', 'not', 'too', 'bad', 'here', 'for', 'a', 'whole', '15', 'minutes', 'he', 'is', 'engaging', 'then', 'goes', 'about', 'the', 'sleepwalking', 'that', 'he', 'has', 'done', 'for', 'his', 'last', '2', 'movies']\n",
      "['whoever', 'is', 'doing', 'his', 'fighting', 'is', 'doing', 'a', 'commendable', 'job']\n",
      "['the', 'light', 'that', 'shines', 'through', 'is', 'tamara', 'davies', 'as', 'colds', 'sidekick']\n",
      "['she', 'has', 'the', 'chopsskillslooks', 'and', 'drive', 'that', 'give', 'black', 'dawn', 'a', 'little', 'kick', 'in', 'the', 'action', 'department']\n",
      "['how', 'she', 'got', 'in', 'this', 'movie', 'is', 'a', 'riddle']\n",
      "['she', 'should', 'be', 'a', 'bigger', 'star']\n",
      "['nicholas', 'davidoff', 'and', 'john', 'pyper', 'feruson', 'are', 'not', 'believable', 'bad', 'guys']\n",
      "['whose', 'idea', 'was', 'it', 'to', 'make', 'a', 'sequel', 'to', 'a', 'bad', 'movie', 'in', 'the', 'first', 'place']\n",
      "['it', 'did', 'not', 'set', 'the', 'world', 'on', 'fire', 'to', 'my', 'memory']\n",
      "['i', 'can', 'only', 'hope', 'they', 'inject', 'the', 'next', 'forigner', 'sequel', 'with', 'some', 'classimaginationless', 'stunt', 'doubles', 'and', 'a', 'livelier', 'seagal']\n",
      "['if', 'not', 'then', 'leave', 'jonathon', 'cold', 'on', 'ice']\n"
     ]
    }
   ],
   "source": [
    "# Print each word in the sentence and identify detais of sentence samples\n",
    "for i in sentences_sample:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the same for all of our train and unlabeled reviews. Here we will join 2 lists into 1. This may take several minutes to complete due to large number of reviews needed to be clean up (75,000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for train set.\n",
      "Done for unlabled set.\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for review in train.review:\n",
    "    sentences += convert_to_sentences(review, tokenizer)\n",
    "print(\"Done for train set.\")\n",
    "\n",
    "for review in unlabeled.review:\n",
    "    sentences += convert_to_sentences(review, tokenizer)\n",
    "print(\"Done for unlabled set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check first 5 sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'ive', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n",
      "\n",
      "['maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent']\n",
      "\n",
      "['moonwalker', 'is', 'part', 'biography', 'part', 'feature', 'film', 'which', 'i', 'remember', 'going', 'to', 'see', 'at', 'the', 'cinema', 'when', 'it', 'was', 'originally', 'released']\n",
      "\n",
      "['some', 'of', 'it', 'has', 'subtle', 'messages', 'about', 'mjs', 'feeling', 'towards', 'the', 'press', 'and', 'also', 'the', 'obvious', 'message', 'of', 'drugs', 'are', 'bad', 'mkay']\n",
      "\n",
      "['visually', 'impressive', 'but', 'of', 'course', 'this', 'is', 'all', 'about', 'michael', 'jackson', 'so', 'unless', 'you', 'remotely', 'like', 'mj', 'in', 'anyway', 'then', 'you', 'are', 'going', 'to', 'hate', 'this', 'and', 'find', 'it', 'boring']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in sentences[0:5]:\n",
    "    print(\"{}\\n\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `word2vec` embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing the embedding, let's define values for the model.\n",
    "\n",
    "- `num_feature`: The dimension of word vector. The more dimension the better representation but this is going to take more time to learn and more data. However, since we don't have that much data, let's set this to 50 only (default is 100).\n",
    "- `min_word_count`: Any words appears less than this number will not be considered in the learning (default is 5).\n",
    "- `window_size`: For any given word, window defines how many words to consider to itâ€™s left and right (default is 5). This is the maximum distance between the current and predicted word within a sentence.\n",
    "- `down_sampling`: The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
    "- `num_thread`: Number of parallel processes to run.\n",
    "- `iteration`: Number of iterations (epochs) over the corpus (default is 5). However, in practice, it's advised that more iteration will improve the representations.\n",
    "- Training algorithm: we will select between CBOW and Skip-gram model.\n",
    "        \n",
    "    - CBOW: works well with small dataset, well representation with rare words/phrases.\n",
    "    - Skip-gram: faster training time, slightly better accuracy for frequent words.\n",
    "    \n",
    "In this case, we will pick CBOW which is the default value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 15:51:27,968 : INFO : collecting all words and their counts\n",
      "2024-02-20 15:51:27,973 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 15:51:28,115 : INFO : PROGRESS: at sentence #10000, processed 186930 words, keeping 17241 word types\n",
      "2024-02-20 15:51:28,334 : INFO : PROGRESS: at sentence #20000, processed 370700 words, keeping 24543 word types\n",
      "2024-02-20 15:51:28,482 : INFO : PROGRESS: at sentence #30000, processed 555660 words, keeping 29804 word types\n",
      "2024-02-20 15:51:28,619 : INFO : PROGRESS: at sentence #40000, processed 738919 words, keeping 34397 word types\n",
      "2024-02-20 15:51:28,766 : INFO : PROGRESS: at sentence #50000, processed 925692 words, keeping 38424 word types\n",
      "2024-02-20 15:51:28,906 : INFO : PROGRESS: at sentence #60000, processed 1106441 words, keeping 41571 word types\n",
      "2024-02-20 15:51:29,049 : INFO : PROGRESS: at sentence #70000, processed 1290985 words, keeping 44601 word types\n",
      "2024-02-20 15:51:29,186 : INFO : PROGRESS: at sentence #80000, processed 1472549 words, keeping 47163 word types\n",
      "2024-02-20 15:51:29,330 : INFO : PROGRESS: at sentence #90000, processed 1657500 words, keeping 49788 word types\n",
      "2024-02-20 15:51:29,469 : INFO : PROGRESS: at sentence #100000, processed 1840856 words, keeping 52186 word types\n",
      "2024-02-20 15:51:29,606 : INFO : PROGRESS: at sentence #110000, processed 2022846 words, keeping 54637 word types\n",
      "2024-02-20 15:51:29,735 : INFO : PROGRESS: at sentence #120000, processed 2207811 words, keeping 56787 word types\n",
      "2024-02-20 15:51:29,861 : INFO : PROGRESS: at sentence #130000, processed 2388456 words, keeping 58767 word types\n",
      "2024-02-20 15:51:29,994 : INFO : PROGRESS: at sentence #140000, processed 2573759 words, keeping 60934 word types\n",
      "2024-02-20 15:51:30,123 : INFO : PROGRESS: at sentence #150000, processed 2755603 words, keeping 62920 word types\n",
      "2024-02-20 15:51:30,251 : INFO : PROGRESS: at sentence #160000, processed 2936688 words, keeping 64557 word types\n",
      "2024-02-20 15:51:30,385 : INFO : PROGRESS: at sentence #170000, processed 3121483 words, keeping 66302 word types\n",
      "2024-02-20 15:51:30,510 : INFO : PROGRESS: at sentence #180000, processed 3304333 words, keeping 67998 word types\n",
      "2024-02-20 15:51:30,646 : INFO : PROGRESS: at sentence #190000, processed 3486449 words, keeping 69714 word types\n",
      "2024-02-20 15:51:30,769 : INFO : PROGRESS: at sentence #200000, processed 3669541 words, keeping 71335 word types\n",
      "2024-02-20 15:51:30,896 : INFO : PROGRESS: at sentence #210000, processed 3848749 words, keeping 72900 word types\n",
      "2024-02-20 15:51:31,031 : INFO : PROGRESS: at sentence #220000, processed 4032182 words, keeping 74387 word types\n",
      "2024-02-20 15:51:31,161 : INFO : PROGRESS: at sentence #230000, processed 4217757 words, keeping 75878 word types\n",
      "2024-02-20 15:51:31,295 : INFO : PROGRESS: at sentence #240000, processed 4403054 words, keeping 77334 word types\n",
      "2024-02-20 15:51:31,432 : INFO : PROGRESS: at sentence #250000, processed 4584843 words, keeping 78826 word types\n",
      "2024-02-20 15:51:31,556 : INFO : PROGRESS: at sentence #260000, processed 4766859 words, keeping 80272 word types\n",
      "2024-02-20 15:51:31,695 : INFO : PROGRESS: at sentence #270000, processed 4951027 words, keeping 81690 word types\n",
      "2024-02-20 15:51:31,846 : INFO : PROGRESS: at sentence #280000, processed 5136434 words, keeping 82992 word types\n",
      "2024-02-20 15:51:31,994 : INFO : PROGRESS: at sentence #290000, processed 5317655 words, keeping 84315 word types\n",
      "2024-02-20 15:51:32,153 : INFO : PROGRESS: at sentence #300000, processed 5493057 words, keeping 85710 word types\n",
      "2024-02-20 15:51:32,322 : INFO : PROGRESS: at sentence #310000, processed 5677666 words, keeping 86974 word types\n",
      "2024-02-20 15:51:32,494 : INFO : PROGRESS: at sentence #320000, processed 5855966 words, keeping 88327 word types\n",
      "2024-02-20 15:51:32,664 : INFO : PROGRESS: at sentence #330000, processed 6044095 words, keeping 89988 word types\n",
      "2024-02-20 15:51:32,844 : INFO : PROGRESS: at sentence #340000, processed 6229697 words, keeping 91727 word types\n",
      "2024-02-20 15:51:33,028 : INFO : PROGRESS: at sentence #350000, processed 6415017 words, keeping 93288 word types\n",
      "2024-02-20 15:51:33,193 : INFO : PROGRESS: at sentence #360000, processed 6600194 words, keeping 94858 word types\n",
      "2024-02-20 15:51:33,309 : INFO : PROGRESS: at sentence #370000, processed 6783351 words, keeping 96418 word types\n",
      "2024-02-20 15:51:33,430 : INFO : PROGRESS: at sentence #380000, processed 6967473 words, keeping 97879 word types\n",
      "2024-02-20 15:51:33,549 : INFO : PROGRESS: at sentence #390000, processed 7149232 words, keeping 99266 word types\n",
      "2024-02-20 15:51:33,665 : INFO : PROGRESS: at sentence #400000, processed 7334962 words, keeping 100745 word types\n",
      "2024-02-20 15:51:33,767 : INFO : PROGRESS: at sentence #410000, processed 7518790 words, keeping 102166 word types\n",
      "2024-02-20 15:51:33,901 : INFO : PROGRESS: at sentence #420000, processed 7702372 words, keeping 103453 word types\n",
      "2024-02-20 15:51:34,015 : INFO : PROGRESS: at sentence #430000, processed 7883197 words, keeping 104782 word types\n",
      "2024-02-20 15:51:34,132 : INFO : PROGRESS: at sentence #440000, processed 8070940 words, keeping 106007 word types\n",
      "2024-02-20 15:51:34,245 : INFO : PROGRESS: at sentence #450000, processed 8255466 words, keeping 107348 word types\n",
      "2024-02-20 15:51:34,364 : INFO : PROGRESS: at sentence #460000, processed 8442387 words, keeping 108773 word types\n",
      "2024-02-20 15:51:34,482 : INFO : PROGRESS: at sentence #470000, processed 8628260 words, keeping 109918 word types\n",
      "2024-02-20 15:51:34,613 : INFO : PROGRESS: at sentence #480000, processed 8810652 words, keeping 111207 word types\n",
      "2024-02-20 15:51:34,747 : INFO : PROGRESS: at sentence #490000, processed 8993812 words, keeping 112283 word types\n",
      "2024-02-20 15:51:34,881 : INFO : PROGRESS: at sentence #500000, processed 9176138 words, keeping 113487 word types\n",
      "2024-02-20 15:51:34,996 : INFO : PROGRESS: at sentence #510000, processed 9357436 words, keeping 114689 word types\n",
      "2024-02-20 15:51:35,141 : INFO : PROGRESS: at sentence #520000, processed 9546688 words, keeping 115804 word types\n",
      "2024-02-20 15:51:35,269 : INFO : PROGRESS: at sentence #530000, processed 9729476 words, keeping 116968 word types\n",
      "2024-02-20 15:51:35,397 : INFO : PROGRESS: at sentence #540000, processed 9914844 words, keeping 118236 word types\n",
      "2024-02-20 15:51:35,547 : INFO : PROGRESS: at sentence #550000, processed 10102452 words, keeping 119504 word types\n",
      "2024-02-20 15:51:35,685 : INFO : PROGRESS: at sentence #560000, processed 10287567 words, keeping 120532 word types\n",
      "2024-02-20 15:51:35,814 : INFO : PROGRESS: at sentence #570000, processed 10477308 words, keeping 121654 word types\n",
      "2024-02-20 15:51:35,930 : INFO : PROGRESS: at sentence #580000, processed 10657549 words, keeping 122805 word types\n",
      "2024-02-20 15:51:36,063 : INFO : PROGRESS: at sentence #590000, processed 10840268 words, keeping 123919 word types\n",
      "2024-02-20 15:51:36,180 : INFO : PROGRESS: at sentence #600000, processed 11022009 words, keeping 125112 word types\n",
      "2024-02-20 15:51:36,298 : INFO : PROGRESS: at sentence #610000, processed 11206690 words, keeping 126218 word types\n",
      "2024-02-20 15:51:36,412 : INFO : PROGRESS: at sentence #620000, processed 11387594 words, keeping 127233 word types\n",
      "2024-02-20 15:51:36,527 : INFO : PROGRESS: at sentence #630000, processed 11573708 words, keeping 128299 word types\n",
      "2024-02-20 15:51:36,638 : INFO : PROGRESS: at sentence #640000, processed 11756840 words, keeping 129354 word types\n",
      "2024-02-20 15:51:36,747 : INFO : PROGRESS: at sentence #650000, processed 11936109 words, keeping 130397 word types\n",
      "2024-02-20 15:51:36,871 : INFO : PROGRESS: at sentence #660000, processed 12122430 words, keeping 131441 word types\n",
      "2024-02-20 15:51:36,982 : INFO : PROGRESS: at sentence #670000, processed 12304703 words, keeping 132468 word types\n",
      "2024-02-20 15:51:37,121 : INFO : PROGRESS: at sentence #680000, processed 12489884 words, keeping 133431 word types\n",
      "2024-02-20 15:51:37,235 : INFO : PROGRESS: at sentence #690000, processed 12675604 words, keeping 134567 word types\n",
      "2024-02-20 15:51:37,363 : INFO : PROGRESS: at sentence #700000, processed 12859129 words, keeping 135561 word types\n",
      "2024-02-20 15:51:37,488 : INFO : PROGRESS: at sentence #710000, processed 13042120 words, keeping 136566 word types\n",
      "2024-02-20 15:51:37,628 : INFO : PROGRESS: at sentence #720000, processed 13227541 words, keeping 137489 word types\n",
      "2024-02-20 15:51:37,759 : INFO : PROGRESS: at sentence #730000, processed 13409069 words, keeping 138626 word types\n",
      "2024-02-20 15:51:37,914 : INFO : PROGRESS: at sentence #740000, processed 13593516 words, keeping 139611 word types\n",
      "2024-02-20 15:51:38,051 : INFO : PROGRESS: at sentence #750000, processed 13775886 words, keeping 140509 word types\n",
      "2024-02-20 15:51:38,197 : INFO : PROGRESS: at sentence #760000, processed 13959387 words, keeping 141433 word types\n",
      "2024-02-20 15:51:38,327 : INFO : PROGRESS: at sentence #770000, processed 14144529 words, keeping 142409 word types\n",
      "2024-02-20 15:51:38,457 : INFO : PROGRESS: at sentence #780000, processed 14330682 words, keeping 143330 word types\n",
      "2024-02-20 15:51:38,565 : INFO : PROGRESS: at sentence #790000, processed 14513341 words, keeping 144365 word types\n",
      "2024-02-20 15:51:38,680 : INFO : PROGRESS: at sentence #800000, processed 14697331 words, keeping 145243 word types\n",
      "2024-02-20 15:51:38,800 : INFO : PROGRESS: at sentence #810000, processed 14883732 words, keeping 146164 word types\n",
      "2024-02-20 15:51:38,915 : INFO : PROGRESS: at sentence #820000, processed 15065908 words, keeping 147049 word types\n",
      "2024-02-20 15:51:39,048 : INFO : PROGRESS: at sentence #830000, processed 15249901 words, keeping 148005 word types\n",
      "2024-02-20 15:51:39,172 : INFO : PROGRESS: at sentence #840000, processed 15438217 words, keeping 148975 word types\n",
      "2024-02-20 15:51:39,280 : INFO : PROGRESS: at sentence #850000, processed 15618964 words, keeping 149897 word types\n",
      "2024-02-20 15:51:39,398 : INFO : PROGRESS: at sentence #860000, processed 15803899 words, keeping 150672 word types\n",
      "2024-02-20 15:51:39,544 : INFO : PROGRESS: at sentence #870000, processed 15989141 words, keeping 151632 word types\n",
      "2024-02-20 15:51:39,662 : INFO : PROGRESS: at sentence #880000, processed 16171382 words, keeping 152480 word types\n",
      "2024-02-20 15:51:39,770 : INFO : PROGRESS: at sentence #890000, processed 16354745 words, keeping 153389 word types\n",
      "2024-02-20 15:51:39,878 : INFO : PROGRESS: at sentence #900000, processed 16536785 words, keeping 154269 word types\n",
      "2024-02-20 15:51:39,988 : INFO : PROGRESS: at sentence #910000, processed 16717436 words, keeping 155100 word types\n",
      "2024-02-20 15:51:40,107 : INFO : PROGRESS: at sentence #920000, processed 16901578 words, keeping 156037 word types\n",
      "2024-02-20 15:51:40,234 : INFO : PROGRESS: at sentence #930000, processed 17090997 words, keeping 156964 word types\n",
      "2024-02-20 15:51:40,347 : INFO : PROGRESS: at sentence #940000, processed 17277575 words, keeping 157806 word types\n",
      "2024-02-20 15:51:40,471 : INFO : PROGRESS: at sentence #950000, processed 17463013 words, keeping 158737 word types\n",
      "2024-02-20 15:51:40,496 : INFO : collected 158916 word types from a corpus of 17503976 raw words and 952321 sentences\n",
      "2024-02-20 15:51:40,502 : INFO : Creating a fresh vocabulary\n",
      "2024-02-20 15:51:40,777 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 24800 unique words (15.605728812706083%% of original 158916, drops 134116)', 'datetime': '2024-02-20T15:51:40.776673', 'gensim': '4.1.2', 'python': '3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-20 15:51:40,779 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 17100338 word corpus (97.69402106127203%% of original 17503976, drops 403638)', 'datetime': '2024-02-20T15:51:40.779573', 'gensim': '4.1.2', 'python': '3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-20 15:51:41,057 : INFO : deleting the raw counts dictionary of 158916 items\n",
      "2024-02-20 15:51:41,079 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2024-02-20 15:51:41,086 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12868120.592538388 word corpus (75.3%% of prior 17100338)', 'datetime': '2024-02-20T15:51:41.086221', 'gensim': '4.1.2', 'python': '3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-20 15:51:41,446 : INFO : estimated required memory for 24800 words and 100 dimensions: 32240000 bytes\n",
      "2024-02-20 15:51:41,448 : INFO : resetting layer weights\n",
      "2024-02-20 15:51:41,480 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-20T15:51:41.480538', 'gensim': '4.1.2', 'python': '3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n",
      "2024-02-20 15:51:41,483 : INFO : Word2Vec lifecycle event {'msg': 'training model with 5 workers on 24800 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2024-02-20T15:51:41.483550', 'gensim': '4.1.2', 'python': '3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2024-02-20 15:51:42,711 : INFO : EPOCH 1 - PROGRESS: at 7.86% examples, 1004982 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:51:43,716 : INFO : EPOCH 1 - PROGRESS: at 16.46% examples, 1050180 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:51:44,720 : INFO : EPOCH 1 - PROGRESS: at 25.50% examples, 1084059 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:51:45,729 : INFO : EPOCH 1 - PROGRESS: at 34.09% examples, 1085165 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:51:46,732 : INFO : EPOCH 1 - PROGRESS: at 42.54% examples, 1086340 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:51:47,731 : INFO : EPOCH 1 - PROGRESS: at 51.31% examples, 1093500 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:51:48,735 : INFO : EPOCH 1 - PROGRESS: at 59.79% examples, 1094136 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:51:49,746 : INFO : EPOCH 1 - PROGRESS: at 67.48% examples, 1078766 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:51:50,748 : INFO : EPOCH 1 - PROGRESS: at 75.82% examples, 1077927 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:51:51,761 : INFO : EPOCH 1 - PROGRESS: at 84.65% examples, 1082371 words/s, in_qsize 10, out_qsize 0\n",
      "2024-02-20 15:51:52,762 : INFO : EPOCH 1 - PROGRESS: at 93.42% examples, 1086680 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:51:53,475 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2024-02-20 15:51:53,482 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2024-02-20 15:51:53,482 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2024-02-20 15:51:53,491 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2024-02-20 15:51:53,497 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-02-20 15:51:53,497 : INFO : EPOCH - 1 : training on 17503976 raw words (12867374 effective words) took 11.8s, 1090945 effective words/s\n",
      "2024-02-20 15:51:54,513 : INFO : EPOCH 2 - PROGRESS: at 8.49% examples, 1091245 words/s, in_qsize 10, out_qsize 0\n",
      "2024-02-20 15:51:55,513 : INFO : EPOCH 2 - PROGRESS: at 16.80% examples, 1078390 words/s, in_qsize 10, out_qsize 0\n",
      "2024-02-20 15:51:56,513 : INFO : EPOCH 2 - PROGRESS: at 25.85% examples, 1105093 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:51:57,513 : INFO : EPOCH 2 - PROGRESS: at 34.93% examples, 1118643 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:51:58,516 : INFO : EPOCH 2 - PROGRESS: at 43.34% examples, 1111730 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:51:59,522 : INFO : EPOCH 2 - PROGRESS: at 51.77% examples, 1106654 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:00,530 : INFO : EPOCH 2 - PROGRESS: at 60.39% examples, 1106682 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:01,533 : INFO : EPOCH 2 - PROGRESS: at 69.31% examples, 1111100 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:02,530 : INFO : EPOCH 2 - PROGRESS: at 77.70% examples, 1106847 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:03,531 : INFO : EPOCH 2 - PROGRESS: at 86.30% examples, 1107103 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:04,531 : INFO : EPOCH 2 - PROGRESS: at 95.08% examples, 1109100 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:05,067 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2024-02-20 15:52:05,067 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2024-02-20 15:52:05,090 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2024-02-20 15:52:05,090 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2024-02-20 15:52:05,090 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-02-20 15:52:05,096 : INFO : EPOCH - 2 : training on 17503976 raw words (12870633 effective words) took 11.6s, 1110932 effective words/s\n",
      "2024-02-20 15:52:06,114 : INFO : EPOCH 3 - PROGRESS: at 8.82% examples, 1131703 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:07,118 : INFO : EPOCH 3 - PROGRESS: at 17.60% examples, 1125600 words/s, in_qsize 10, out_qsize 0\n",
      "2024-02-20 15:52:08,117 : INFO : EPOCH 3 - PROGRESS: at 26.02% examples, 1108452 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:09,114 : INFO : EPOCH 3 - PROGRESS: at 34.58% examples, 1103135 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:10,133 : INFO : EPOCH 3 - PROGRESS: at 43.00% examples, 1098836 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:11,134 : INFO : EPOCH 3 - PROGRESS: at 51.71% examples, 1102545 words/s, in_qsize 8, out_qsize 1\n",
      "2024-02-20 15:52:12,135 : INFO : EPOCH 3 - PROGRESS: at 60.73% examples, 1111233 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:13,150 : INFO : EPOCH 3 - PROGRESS: at 69.95% examples, 1118300 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:14,151 : INFO : EPOCH 3 - PROGRESS: at 79.13% examples, 1124293 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:15,161 : INFO : EPOCH 3 - PROGRESS: at 87.72% examples, 1122322 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:16,160 : INFO : EPOCH 3 - PROGRESS: at 96.86% examples, 1127039 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:16,490 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2024-02-20 15:52:16,500 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2024-02-20 15:52:16,505 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2024-02-20 15:52:16,520 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2024-02-20 15:52:16,520 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-02-20 15:52:16,523 : INFO : EPOCH - 3 : training on 17503976 raw words (12867897 effective words) took 11.4s, 1127317 effective words/s\n",
      "2024-02-20 15:52:17,536 : INFO : EPOCH 4 - PROGRESS: at 8.65% examples, 1116215 words/s, in_qsize 10, out_qsize 1\n",
      "2024-02-20 15:52:18,543 : INFO : EPOCH 4 - PROGRESS: at 17.14% examples, 1099913 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:19,550 : INFO : EPOCH 4 - PROGRESS: at 26.19% examples, 1115850 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:20,554 : INFO : EPOCH 4 - PROGRESS: at 35.21% examples, 1124856 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:21,548 : INFO : EPOCH 4 - PROGRESS: at 44.15% examples, 1129469 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:22,554 : INFO : EPOCH 4 - PROGRESS: at 53.14% examples, 1133974 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:23,554 : INFO : EPOCH 4 - PROGRESS: at 62.09% examples, 1137630 words/s, in_qsize 8, out_qsize 1\n",
      "2024-02-20 15:52:24,559 : INFO : EPOCH 4 - PROGRESS: at 71.26% examples, 1142001 words/s, in_qsize 8, out_qsize 1\n",
      "2024-02-20 15:52:25,556 : INFO : EPOCH 4 - PROGRESS: at 80.04% examples, 1140682 words/s, in_qsize 8, out_qsize 1\n",
      "2024-02-20 15:52:26,569 : INFO : EPOCH 4 - PROGRESS: at 89.09% examples, 1142696 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:27,571 : INFO : EPOCH 4 - PROGRESS: at 97.80% examples, 1140098 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:27,799 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2024-02-20 15:52:27,806 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2024-02-20 15:52:27,816 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2024-02-20 15:52:27,816 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2024-02-20 15:52:27,816 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-02-20 15:52:27,816 : INFO : EPOCH - 4 : training on 17503976 raw words (12868656 effective words) took 11.3s, 1140670 effective words/s\n",
      "2024-02-20 15:52:28,839 : INFO : EPOCH 5 - PROGRESS: at 8.55% examples, 1097376 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:29,839 : INFO : EPOCH 5 - PROGRESS: at 17.31% examples, 1108448 words/s, in_qsize 9, out_qsize 1\n",
      "2024-02-20 15:52:30,850 : INFO : EPOCH 5 - PROGRESS: at 26.02% examples, 1106323 words/s, in_qsize 10, out_qsize 1\n",
      "2024-02-20 15:52:31,853 : INFO : EPOCH 5 - PROGRESS: at 35.26% examples, 1124500 words/s, in_qsize 10, out_qsize 0\n",
      "2024-02-20 15:52:32,851 : INFO : EPOCH 5 - PROGRESS: at 43.92% examples, 1122601 words/s, in_qsize 10, out_qsize 0\n",
      "2024-02-20 15:52:33,856 : INFO : EPOCH 5 - PROGRESS: at 52.86% examples, 1127246 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:34,854 : INFO : EPOCH 5 - PROGRESS: at 61.24% examples, 1121030 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:35,863 : INFO : EPOCH 5 - PROGRESS: at 70.23% examples, 1124737 words/s, in_qsize 10, out_qsize 0\n",
      "2024-02-20 15:52:36,871 : INFO : EPOCH 5 - PROGRESS: at 78.67% examples, 1119936 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:37,873 : INFO : EPOCH 5 - PROGRESS: at 87.61% examples, 1122234 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:38,866 : INFO : EPOCH 5 - PROGRESS: at 96.52% examples, 1124286 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 15:52:39,256 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2024-02-20 15:52:39,267 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2024-02-20 15:52:39,272 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2024-02-20 15:52:39,272 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2024-02-20 15:52:39,277 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-02-20 15:52:39,277 : INFO : EPOCH - 5 : training on 17503976 raw words (12868418 effective words) took 11.4s, 1124659 effective words/s\n",
      "2024-02-20 15:52:39,280 : INFO : Word2Vec lifecycle event {'msg': 'training on 87519880 raw words (64342978 effective words) took 57.8s, 1113309 effective words/s', 'datetime': '2024-02-20T15:52:39.280069', 'gensim': '4.1.2', 'python': '3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2024-02-20 15:52:39,281 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=24800, vector_size=100, alpha=0.025)', 'datetime': '2024-02-20T15:52:39.281866', 'gensim': '4.1.2', 'python': '3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# Applying word2vec on all the sentences we have converted all the sentences that are present it our dataset into vector format\n",
    "num_feature = 50\n",
    "min_word_count = 20\n",
    "num_thread = 5\n",
    "window_size = 10\n",
    "down_sampling = 0.001\n",
    "iteration = 20\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = word2vec.Word2Vec(sentences, \n",
    "                          min_count = min_word_count, \n",
    "                          window = window_size, \n",
    "                          sample = down_sampling, \n",
    "                          workers=num_thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total training time for `word2vec` embeddings is 851.8s.\n",
    "Let's save model for backing up purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 16:46:28,904 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'gensim_word2vec_model_180518', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-02-20T16:46:28.904089', 'gensim': '4.1.2', 'python': '3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'saving'}\n",
      "2024-02-20 16:46:28,904 : INFO : not storing attribute cum_table\n",
      "2024-02-20 16:46:28,949 : INFO : saved gensim_word2vec_model_180518\n"
     ]
    }
   ],
   "source": [
    "# Save the model created\n",
    "model.save(\"gensim_word2vec_model_180518\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the saved model, we use below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 16:46:39,914 : INFO : loading Word2Vec object from gensim_word2vec_model_180518\n",
      "2024-02-20 16:46:39,960 : INFO : loading wv recursively from gensim_word2vec_model_180518.wv.* with mmap=None\n",
      "2024-02-20 16:46:39,961 : INFO : setting ignored attribute cum_table to None\n",
      "2024-02-20 16:46:40,101 : INFO : Word2Vec lifecycle event {'fname': 'gensim_word2vec_model_180518', 'datetime': '2024-02-20T16:46:40.101910', 'gensim': '4.1.2', 'python': '3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = Word2Vec.load(\"gensim_word2vec_model_180518\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model checking\n",
    "\n",
    "Our embedding is ready now. Let's perform some NLP task with the model. There are about 24,800 words in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of words: 24800\n"
     ]
    }
   ],
   "source": [
    "# Check the total number of words in the model\n",
    "print(\"Total of words: {}\".format(len(model.wv.index_to_key)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word similarity\n",
    "\n",
    "In the example below, we can easily see that numbers `1` and `2` tend to stay near each other than the words `apple` and `weather`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7546308\n"
     ]
    }
   ],
   "source": [
    "# Find the similarity between 1 and 2 how close to each other they are present.\n",
    "print(model.wv.similarity('1', '2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28807396\n"
     ]
    }
   ],
   "source": [
    "# Find the similarity between apple and weather how close to each other they are present.\n",
    "print(model.wv.similarity('apple', 'weather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('protagonist', 0.6902007460594177),\n",
       " ('role', 0.6755948066711426),\n",
       " ('villain', 0.6517958045005798),\n",
       " ('personality', 0.6343037486076355),\n",
       " ('characters', 0.6324491500854492),\n",
       " ('attraction', 0.5448999404907227),\n",
       " ('actor', 0.5357121229171753),\n",
       " ('persona', 0.5354807376861572),\n",
       " ('antagonist', 0.5153608918190002),\n",
       " ('demeanor', 0.5046439170837402)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar words that are present next to character, and we get the following results.\n",
    "model.wv.most_similar('character')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have `good:best :: bad:?` and we want to find the missing word (which is `worse`). We have to look for  vector(x) such that it is close to:\n",
    "    \n",
    "    vector(`best`) - vector(`good`) + vector(`bad`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', 0.7701299786567688),\n",
       " ('funniest', 0.66257643699646),\n",
       " ('finest', 0.626103937625885),\n",
       " ('weakest', 0.6009334921836853),\n",
       " ('poorest', 0.5990979671478271),\n",
       " ('greatest', 0.5783557891845703),\n",
       " ('scariest', 0.5781227946281433),\n",
       " ('stupidest', 0.5666341781616211),\n",
       " ('lamest', 0.5537522435188293),\n",
       " ('cheesiest', 0.5401484370231628)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the missing word when given positive and negative inputs example 1.\n",
    "model.wv.most_similar(positive=['best','bad'],negative=['good'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is another example. The output is exactly as in our expectation - the word `paris`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paris', 0.7268273234367371),\n",
       " ('vienna', 0.7151327729225159),\n",
       " ('vermont', 0.7054781317710876),\n",
       " ('du', 0.6951680183410645),\n",
       " ('tudor', 0.690239667892456),\n",
       " ('rio', 0.6882959604263306),\n",
       " ('virginia', 0.6670973300933838),\n",
       " ('georgia', 0.6643968820571899),\n",
       " ('london', 0.6642588376998901),\n",
       " ('18th', 0.663261890411377)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Find the missing word when given positive and negative inputs example 2.\n",
    "model.wv.most_similar(positive=['berlin','france'],negative=['germany'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different from the group\n",
    "\n",
    "By using `doesnt_match` function, we can find out which word does not belong to the group of words. Since 3 out of 4 are negative words, leaving `good` the only positive word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "# Find out which word dose not belog to a perticular group of words.\n",
    "print(model.wv.doesnt_match(\"terrible bad horrible good\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word vectors\n",
    "\n",
    "We can get vector representations of a specific word easily by using the function `get_vector`. Below are some vectors as examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for word 'film': \n",
      "[-0.7503893   3.3117137  -0.08055668  3.3314981  -3.9168468   4.0740614\n",
      "  1.720352    0.17729267 -2.9377303  -0.3530059  -0.26996315 -0.8304131\n",
      "  2.3925884  -0.82503444  1.7596908  -2.4348834   0.5046636   1.2159004\n",
      " -1.3662195  -0.24275813 -2.9849536  -0.52368295 -3.0727255  -0.1403945\n",
      "  0.5905988  -1.5283539  -1.2884144   2.6366818   3.2392526  -2.588227\n",
      " -2.2030482   1.5332707  -1.6776215   2.0891924  -2.864895   -2.3103518\n",
      "  2.1927185   0.22805344 -3.9385703   1.7084055   3.2778258   0.57050663\n",
      "  0.12756376 -0.7861977   1.4626007   0.2759785  -2.011784    0.6770213\n",
      " -1.9714171  -2.132479    0.66350836  1.0172902   2.44041     1.5059102\n",
      " -2.255609   -2.001187   -0.02927517  2.4219036   2.234259    4.323884\n",
      "  2.4221594  -6.0610523  -0.06094478  1.0150019  -0.13281061  0.02484169\n",
      " -1.5710665   3.1357245  -1.3091756  -3.9651773  -1.7298561   1.3895074\n",
      " -0.38656276  0.8062439  -0.39627022  1.2938296   2.468507   -0.64202166\n",
      "  0.30750096 -0.31411654  1.808364   -1.432197   -0.27773532 -1.5566807\n",
      " -0.21277106  2.1561594   1.4113932  -2.150003    0.21115285  0.4635713\n",
      " -2.0691946   2.404011    2.584557   -2.0385218   2.0813663   0.2274565\n",
      "  0.87288874  2.200247    1.2593441  -2.175003  ]\n"
     ]
    }
   ],
   "source": [
    "# Get a vector representation of any particular word - 'film'\n",
    "print(\"Vector for word 'film': \\n{}\".format(model.wv.get_vector('film')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for word 'movie': \n",
      "[-5.7691991e-01  1.1479995e+00 -1.0233791e+00  3.4909616e+00\n",
      " -3.1608334e+00  4.2682137e+00  3.6045623e-01  2.4319756e-01\n",
      " -2.6696441e+00 -1.8097198e-01  5.2845532e-01 -2.0128448e-01\n",
      "  1.6378982e+00 -5.8694518e-01  1.4338499e+00 -2.1019242e+00\n",
      "  4.9284998e-01  1.5599576e+00 -7.8196257e-01  2.1106923e+00\n",
      " -1.8566707e+00 -1.2288387e+00 -4.6938748e+00 -7.7675253e-02\n",
      "  5.3333765e-01 -1.6631466e+00 -9.0212572e-01  2.3264823e+00\n",
      "  3.3582087e+00 -2.9959867e+00 -2.4594772e+00  1.0541623e+00\n",
      " -2.4222088e+00  3.1137607e+00 -3.0317674e+00 -2.6605301e+00\n",
      "  1.9669440e+00  3.9107588e-01 -3.6743505e+00  3.1654711e+00\n",
      "  2.9152694e+00 -3.3426475e-02 -6.2896848e-01  4.2351496e-01\n",
      "  9.7219682e-01  7.5878066e-01 -1.1812390e+00 -1.6225159e-01\n",
      " -2.2522054e+00 -2.5029373e+00  8.4004670e-01  1.4362350e+00\n",
      "  2.5612805e+00  4.4987008e-01 -2.9342649e+00 -1.9887673e+00\n",
      " -9.1348392e-01  2.7184153e+00  2.3170495e+00  3.2989283e+00\n",
      "  1.9566458e+00 -5.8452253e+00  9.2251801e-01  1.1729634e+00\n",
      " -1.1211287e+00 -7.3248285e-01 -2.3708401e+00  2.9384985e+00\n",
      " -1.7701000e+00 -5.3699775e+00 -1.5744734e+00  1.3980651e+00\n",
      " -1.5410173e+00  1.3473558e-01 -1.4214301e+00  2.4773362e+00\n",
      "  1.8966595e+00 -1.2876123e+00  3.7288144e-01  4.8620361e-03\n",
      "  1.8740880e+00 -1.3661232e+00  6.3040815e-02 -2.1574752e+00\n",
      "  5.3301454e-01  1.4842504e+00  1.3458352e-01 -9.2144328e-01\n",
      " -1.5492222e-01  1.0794661e+00 -1.9854760e+00  2.1048338e+00\n",
      "  2.1838849e+00 -2.3898215e+00  2.1516621e+00  2.2074394e+00\n",
      " -2.0827094e-02  3.9162509e+00  4.3789080e-01 -2.6040444e+00]\n"
     ]
    }
   ],
   "source": [
    "# Get a vector representation of any particular word - 'movie'\n",
    "print(\"Vector for word 'movie': \\n{}\".format(model.wv['movie']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word index\n",
    "\n",
    "Another useful function named `index2word` can help us find the word at specific index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At index 15 is the word: movie\n"
     ]
    }
   ],
   "source": [
    "# Find a word at a specific index location eg. 15\n",
    "print(\"At index 15 is the word: {}\".format(model.wv.index_to_key[15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index of word\n",
    "\n",
    "By looking up a word in the vocabulary, we can get its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of word 'movie' is: 15\n"
     ]
    }
   ],
   "source": [
    "# If we need to find the index of the particular word in the vocab.\n",
    "word = 'movie'\n",
    "if word in model.wv.key_to_index:\n",
    "    print(\"Index of word '{}' is: {}\".format(word, model.wv.key_to_index[word]))\n",
    "else:\n",
    "    print(\"Word '{}' not found in the vocabulary.\".format(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving model into array of words and vectors\n",
    "\n",
    "In order to make it easier for us to manipulate with the words in vocab and their vectors, we will save them into 2 seperate arrays, one for words and one for vectors.\n",
    "\n",
    "First, create a zero array with same dimension with our embedding (50), but the number of rows will be increased by 2 because we need 1 row for padding character '-' at the beginning and another for 'unk' (use for unknown word) at the end. The reason of doing this is to limit the length of the sequence and anything less than our max length will be padded with '-' with 0 vector values, we choose '-' because in our vocab there is no character like that, causing no confusion with other words when training the RNN model. For 'unk', any words that is not found in the list will take index of 'unk' also have 0 vector value.\n",
    "\n",
    "Then looping through all vocabulary and add corresponding word into `word_list` and vector into `word_vector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The words in the vocabulary and the respective vectors are saved into 2 different arrays for better manipulation.\n",
    "\n",
    "vec_dim = 100  # Assuming the word vectors have a dimensionality of 100\n",
    "word_list = []\n",
    "word_vector = np.zeros((len(model.wv) + 2, vec_dim))  # add '-' as padding and 'unk' as unknown\n",
    "# Add padding value '-'\n",
    "word_list.append('-')  \n",
    "# Iterate through the word vectors\n",
    "for i, word in enumerate(model.wv.index_to_key):\n",
    "    vector = model.wv.get_vector(word)\n",
    "    word_vector[i + 1] = vector\n",
    "    word_list.append(word)\n",
    "# Add 'unk' as unknown\n",
    "word_list.append('unk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in list: 24802\n",
      "Shape of word vector: (24802, 100)\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words in the word_list and word_vector shape.\n",
    "\n",
    "print(\"Number of words in list: {}\".format(len(word_list)))\n",
    "print(\"Shape of word vector: {}\".format(word_vector.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in our vocab: -\n",
      "\n",
      "Its vector values: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Get the first word from the vocab list and its respective vector value.\n",
    "\n",
    "print(\"First word in our vocab: {}\\n\".format(word_list[0]))\n",
    "print(\"Its vector values: \\n{}\".format(word_vector[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last word in our vocab: unk\n",
      "Its vector values: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Get the last word from the vocab list and its respective vector value.\n",
    "\n",
    "print(\"Last word in our vocab: {}\".format(word_list[-1]))\n",
    "print(\"Its vector values: \\n{}\".format(word_vector[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to save them for later use. (We already saved them into `data` folder.)\n",
    "\n",
    "`np.save('./data/word_list_gensim_w2v', word_list)\n",
    "np.save('./data/word_vector_gensim_w2v', word_vector)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load them, we use below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word list and word vector.\n",
    "\n",
    "load_word_list = word_list\n",
    "load_word_vector = word_vector\n",
    "\n",
    "load_word_list = load_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to check if our saved values are consistent with the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the vector of the word 'movie' in our model and try to compare them with the same word in our saved list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of word 'movie' in word_list: 16\n",
      "Vector of word 'movie':\n",
      "[-5.76919913e-01  1.14799953e+00 -1.02337909e+00  3.49096155e+00\n",
      " -3.16083336e+00  4.26821375e+00  3.60456228e-01  2.43197560e-01\n",
      " -2.66964412e+00 -1.80971980e-01  5.28455317e-01 -2.01284483e-01\n",
      "  1.63789821e+00 -5.86945176e-01  1.43384993e+00 -2.10192418e+00\n",
      "  4.92849976e-01  1.55995762e+00 -7.81962574e-01  2.11069226e+00\n",
      " -1.85667074e+00 -1.22883868e+00 -4.69387484e+00 -7.76752532e-02\n",
      "  5.33337653e-01 -1.66314662e+00 -9.02125716e-01  2.32648230e+00\n",
      "  3.35820866e+00 -2.99598670e+00 -2.45947719e+00  1.05416226e+00\n",
      " -2.42220879e+00  3.11376071e+00 -3.03176737e+00 -2.66053009e+00\n",
      "  1.96694398e+00  3.91075879e-01 -3.67435050e+00  3.16547108e+00\n",
      "  2.91526937e+00 -3.34264748e-02 -6.28968477e-01  4.23514962e-01\n",
      "  9.72196817e-01  7.58780658e-01 -1.18123901e+00 -1.62251592e-01\n",
      " -2.25220537e+00 -2.50293732e+00  8.40046704e-01  1.43623495e+00\n",
      "  2.56128049e+00  4.49870080e-01 -2.93426490e+00 -1.98876727e+00\n",
      " -9.13483918e-01  2.71841526e+00  2.31704950e+00  3.29892826e+00\n",
      "  1.95664585e+00 -5.84522533e+00  9.22518015e-01  1.17296338e+00\n",
      " -1.12112868e+00 -7.32482851e-01 -2.37084007e+00  2.93849850e+00\n",
      " -1.77010000e+00 -5.36997747e+00 -1.57447338e+00  1.39806509e+00\n",
      " -1.54101729e+00  1.34735584e-01 -1.42143011e+00  2.47733617e+00\n",
      "  1.89665949e+00 -1.28761232e+00  3.72881442e-01  4.86203609e-03\n",
      "  1.87408805e+00 -1.36612320e+00  6.30408153e-02 -2.15747523e+00\n",
      "  5.33014536e-01  1.48425043e+00  1.34583518e-01 -9.21443284e-01\n",
      " -1.54922217e-01  1.07946610e+00 -1.98547602e+00  2.10483384e+00\n",
      "  2.18388486e+00 -2.38982153e+00  2.15166211e+00  2.20743942e+00\n",
      " -2.08270941e-02  3.91625094e+00  4.37890798e-01 -2.60404444e+00]\n"
     ]
    }
   ],
   "source": [
    "# Check the values and compare with the original model values.\n",
    "\n",
    "print(\"Index of word 'movie' in word_list: {}\".format(load_word_list.index(\"movie\")))\n",
    "print(\"Vector of word 'movie':\\n{}\".format(load_word_vector[16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If vector of word `movie` in the model is the same with vector in our `word_vector` list, the expected result of the subtraction will be 0 since they are the same which also means that our list is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhave\\AppData\\Local\\Temp\\ipykernel_29572\\3840472118.py:1: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  print(\"Result: \\n{}\".format(model.wv.word_vec('movie') - load_word_vector[16]))\n"
     ]
    }
   ],
   "source": [
    "# Results of the loaded word vector and the model word vector for the word 'movie' should give us the results as 0 when subtracted from each other.\n",
    "\n",
    "print(\"Result: \\n{}\".format(model.wv.word_vec('movie') - load_word_vector[16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## `fastText` embedding\n",
    "\n",
    "Next, let's try to learn embedding using `fastText` algorithm with Gensim. We will use the same data prepared above and train the embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 17:39:09,509 : INFO : collecting all words and their counts\n",
      "2024-02-20 17:39:09,509 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-02-20 17:39:09,550 : INFO : PROGRESS: at sentence #10000, processed 186930 words, keeping 17241 word types\n",
      "2024-02-20 17:39:09,591 : INFO : PROGRESS: at sentence #20000, processed 370700 words, keeping 24543 word types\n",
      "2024-02-20 17:39:09,631 : INFO : PROGRESS: at sentence #30000, processed 555660 words, keeping 29804 word types\n",
      "2024-02-20 17:39:09,674 : INFO : PROGRESS: at sentence #40000, processed 738919 words, keeping 34397 word types\n",
      "2024-02-20 17:39:09,723 : INFO : PROGRESS: at sentence #50000, processed 925692 words, keeping 38424 word types\n",
      "2024-02-20 17:39:09,757 : INFO : PROGRESS: at sentence #60000, processed 1106441 words, keeping 41571 word types\n",
      "2024-02-20 17:39:09,823 : INFO : PROGRESS: at sentence #70000, processed 1290985 words, keeping 44601 word types\n",
      "2024-02-20 17:39:09,877 : INFO : PROGRESS: at sentence #80000, processed 1472549 words, keeping 47163 word types\n",
      "2024-02-20 17:39:09,935 : INFO : PROGRESS: at sentence #90000, processed 1657500 words, keeping 49788 word types\n",
      "2024-02-20 17:39:09,991 : INFO : PROGRESS: at sentence #100000, processed 1840856 words, keeping 52186 word types\n",
      "2024-02-20 17:39:10,047 : INFO : PROGRESS: at sentence #110000, processed 2022846 words, keeping 54637 word types\n",
      "2024-02-20 17:39:10,089 : INFO : PROGRESS: at sentence #120000, processed 2207811 words, keeping 56787 word types\n",
      "2024-02-20 17:39:10,166 : INFO : PROGRESS: at sentence #130000, processed 2388456 words, keeping 58767 word types\n",
      "2024-02-20 17:39:10,206 : INFO : PROGRESS: at sentence #140000, processed 2573759 words, keeping 60934 word types\n",
      "2024-02-20 17:39:10,293 : INFO : PROGRESS: at sentence #150000, processed 2755603 words, keeping 62920 word types\n",
      "2024-02-20 17:39:10,374 : INFO : PROGRESS: at sentence #160000, processed 2936688 words, keeping 64557 word types\n",
      "2024-02-20 17:39:10,422 : INFO : PROGRESS: at sentence #170000, processed 3121483 words, keeping 66302 word types\n",
      "2024-02-20 17:39:10,476 : INFO : PROGRESS: at sentence #180000, processed 3304333 words, keeping 67998 word types\n",
      "2024-02-20 17:39:10,526 : INFO : PROGRESS: at sentence #190000, processed 3486449 words, keeping 69714 word types\n",
      "2024-02-20 17:39:10,578 : INFO : PROGRESS: at sentence #200000, processed 3669541 words, keeping 71335 word types\n",
      "2024-02-20 17:39:10,654 : INFO : PROGRESS: at sentence #210000, processed 3848749 words, keeping 72900 word types\n",
      "2024-02-20 17:39:10,697 : INFO : PROGRESS: at sentence #220000, processed 4032182 words, keeping 74387 word types\n",
      "2024-02-20 17:39:10,740 : INFO : PROGRESS: at sentence #230000, processed 4217757 words, keeping 75878 word types\n",
      "2024-02-20 17:39:10,788 : INFO : PROGRESS: at sentence #240000, processed 4403054 words, keeping 77334 word types\n",
      "2024-02-20 17:39:10,826 : INFO : PROGRESS: at sentence #250000, processed 4584843 words, keeping 78826 word types\n",
      "2024-02-20 17:39:10,873 : INFO : PROGRESS: at sentence #260000, processed 4766859 words, keeping 80272 word types\n",
      "2024-02-20 17:39:10,921 : INFO : PROGRESS: at sentence #270000, processed 4951027 words, keeping 81690 word types\n",
      "2024-02-20 17:39:10,972 : INFO : PROGRESS: at sentence #280000, processed 5136434 words, keeping 82992 word types\n",
      "2024-02-20 17:39:11,009 : INFO : PROGRESS: at sentence #290000, processed 5317655 words, keeping 84315 word types\n",
      "2024-02-20 17:39:11,059 : INFO : PROGRESS: at sentence #300000, processed 5493057 words, keeping 85710 word types\n",
      "2024-02-20 17:39:11,089 : INFO : PROGRESS: at sentence #310000, processed 5677666 words, keeping 86974 word types\n",
      "2024-02-20 17:39:11,150 : INFO : PROGRESS: at sentence #320000, processed 5855966 words, keeping 88327 word types\n",
      "2024-02-20 17:39:11,192 : INFO : PROGRESS: at sentence #330000, processed 6044095 words, keeping 89988 word types\n",
      "2024-02-20 17:39:11,261 : INFO : PROGRESS: at sentence #340000, processed 6229697 words, keeping 91727 word types\n",
      "2024-02-20 17:39:11,312 : INFO : PROGRESS: at sentence #350000, processed 6415017 words, keeping 93288 word types\n",
      "2024-02-20 17:39:11,356 : INFO : PROGRESS: at sentence #360000, processed 6600194 words, keeping 94858 word types\n",
      "2024-02-20 17:39:11,404 : INFO : PROGRESS: at sentence #370000, processed 6783351 words, keeping 96418 word types\n",
      "2024-02-20 17:39:11,440 : INFO : PROGRESS: at sentence #380000, processed 6967473 words, keeping 97879 word types\n",
      "2024-02-20 17:39:11,496 : INFO : PROGRESS: at sentence #390000, processed 7149232 words, keeping 99266 word types\n",
      "2024-02-20 17:39:11,545 : INFO : PROGRESS: at sentence #400000, processed 7334962 words, keeping 100745 word types\n",
      "2024-02-20 17:39:11,593 : INFO : PROGRESS: at sentence #410000, processed 7518790 words, keeping 102166 word types\n",
      "2024-02-20 17:39:11,639 : INFO : PROGRESS: at sentence #420000, processed 7702372 words, keeping 103453 word types\n",
      "2024-02-20 17:39:11,673 : INFO : PROGRESS: at sentence #430000, processed 7883197 words, keeping 104782 word types\n",
      "2024-02-20 17:39:11,731 : INFO : PROGRESS: at sentence #440000, processed 8070940 words, keeping 106007 word types\n",
      "2024-02-20 17:39:11,774 : INFO : PROGRESS: at sentence #450000, processed 8255466 words, keeping 107348 word types\n",
      "2024-02-20 17:39:11,824 : INFO : PROGRESS: at sentence #460000, processed 8442387 words, keeping 108773 word types\n",
      "2024-02-20 17:39:11,873 : INFO : PROGRESS: at sentence #470000, processed 8628260 words, keeping 109918 word types\n",
      "2024-02-20 17:39:11,942 : INFO : PROGRESS: at sentence #480000, processed 8810652 words, keeping 111207 word types\n",
      "2024-02-20 17:39:11,995 : INFO : PROGRESS: at sentence #490000, processed 8993812 words, keeping 112283 word types\n",
      "2024-02-20 17:39:12,039 : INFO : PROGRESS: at sentence #500000, processed 9176138 words, keeping 113487 word types\n",
      "2024-02-20 17:39:12,090 : INFO : PROGRESS: at sentence #510000, processed 9357436 words, keeping 114689 word types\n",
      "2024-02-20 17:39:12,123 : INFO : PROGRESS: at sentence #520000, processed 9546688 words, keeping 115804 word types\n",
      "2024-02-20 17:39:12,180 : INFO : PROGRESS: at sentence #530000, processed 9729476 words, keeping 116968 word types\n",
      "2024-02-20 17:39:12,226 : INFO : PROGRESS: at sentence #540000, processed 9914844 words, keeping 118236 word types\n",
      "2024-02-20 17:39:12,273 : INFO : PROGRESS: at sentence #550000, processed 10102452 words, keeping 119504 word types\n",
      "2024-02-20 17:39:12,321 : INFO : PROGRESS: at sentence #560000, processed 10287567 words, keeping 120532 word types\n",
      "2024-02-20 17:39:12,374 : INFO : PROGRESS: at sentence #570000, processed 10477308 words, keeping 121654 word types\n",
      "2024-02-20 17:39:12,406 : INFO : PROGRESS: at sentence #580000, processed 10657549 words, keeping 122805 word types\n",
      "2024-02-20 17:39:12,466 : INFO : PROGRESS: at sentence #590000, processed 10840268 words, keeping 123919 word types\n",
      "2024-02-20 17:39:12,505 : INFO : PROGRESS: at sentence #600000, processed 11022009 words, keeping 125112 word types\n",
      "2024-02-20 17:39:12,556 : INFO : PROGRESS: at sentence #610000, processed 11206690 words, keeping 126218 word types\n",
      "2024-02-20 17:39:12,592 : INFO : PROGRESS: at sentence #620000, processed 11387594 words, keeping 127233 word types\n",
      "2024-02-20 17:39:12,654 : INFO : PROGRESS: at sentence #630000, processed 11573708 words, keeping 128299 word types\n",
      "2024-02-20 17:39:12,690 : INFO : PROGRESS: at sentence #640000, processed 11756840 words, keeping 129354 word types\n",
      "2024-02-20 17:39:12,758 : INFO : PROGRESS: at sentence #650000, processed 11936109 words, keeping 130397 word types\n",
      "2024-02-20 17:39:12,824 : INFO : PROGRESS: at sentence #660000, processed 12122430 words, keeping 131441 word types\n",
      "2024-02-20 17:39:12,871 : INFO : PROGRESS: at sentence #670000, processed 12304703 words, keeping 132468 word types\n",
      "2024-02-20 17:39:12,908 : INFO : PROGRESS: at sentence #680000, processed 12489884 words, keeping 133431 word types\n",
      "2024-02-20 17:39:12,961 : INFO : PROGRESS: at sentence #690000, processed 12675604 words, keeping 134567 word types\n",
      "2024-02-20 17:39:13,005 : INFO : PROGRESS: at sentence #700000, processed 12859129 words, keeping 135561 word types\n",
      "2024-02-20 17:39:13,059 : INFO : PROGRESS: at sentence #710000, processed 13042120 words, keeping 136566 word types\n",
      "2024-02-20 17:39:13,113 : INFO : PROGRESS: at sentence #720000, processed 13227541 words, keeping 137489 word types\n",
      "2024-02-20 17:39:13,158 : INFO : PROGRESS: at sentence #730000, processed 13409069 words, keeping 138626 word types\n",
      "2024-02-20 17:39:13,194 : INFO : PROGRESS: at sentence #740000, processed 13593516 words, keeping 139611 word types\n",
      "2024-02-20 17:39:13,247 : INFO : PROGRESS: at sentence #750000, processed 13775886 words, keeping 140509 word types\n",
      "2024-02-20 17:39:13,293 : INFO : PROGRESS: at sentence #760000, processed 13959387 words, keeping 141433 word types\n",
      "2024-02-20 17:39:13,355 : INFO : PROGRESS: at sentence #770000, processed 14144529 words, keeping 142409 word types\n",
      "2024-02-20 17:39:13,440 : INFO : PROGRESS: at sentence #780000, processed 14330682 words, keeping 143330 word types\n",
      "2024-02-20 17:39:13,508 : INFO : PROGRESS: at sentence #790000, processed 14513341 words, keeping 144365 word types\n",
      "2024-02-20 17:39:13,588 : INFO : PROGRESS: at sentence #800000, processed 14697331 words, keeping 145243 word types\n",
      "2024-02-20 17:39:13,658 : INFO : PROGRESS: at sentence #810000, processed 14883732 words, keeping 146164 word types\n",
      "2024-02-20 17:39:13,723 : INFO : PROGRESS: at sentence #820000, processed 15065908 words, keeping 147049 word types\n",
      "2024-02-20 17:39:13,758 : INFO : PROGRESS: at sentence #830000, processed 15249901 words, keeping 148005 word types\n",
      "2024-02-20 17:39:13,806 : INFO : PROGRESS: at sentence #840000, processed 15438217 words, keeping 148975 word types\n",
      "2024-02-20 17:39:13,866 : INFO : PROGRESS: at sentence #850000, processed 15618964 words, keeping 149897 word types\n",
      "2024-02-20 17:39:13,908 : INFO : PROGRESS: at sentence #860000, processed 15803899 words, keeping 150672 word types\n",
      "2024-02-20 17:39:13,959 : INFO : PROGRESS: at sentence #870000, processed 15989141 words, keeping 151632 word types\n",
      "2024-02-20 17:39:14,008 : INFO : PROGRESS: at sentence #880000, processed 16171382 words, keeping 152480 word types\n",
      "2024-02-20 17:39:14,055 : INFO : PROGRESS: at sentence #890000, processed 16354745 words, keeping 153389 word types\n",
      "2024-02-20 17:39:14,104 : INFO : PROGRESS: at sentence #900000, processed 16536785 words, keeping 154269 word types\n",
      "2024-02-20 17:39:14,146 : INFO : PROGRESS: at sentence #910000, processed 16717436 words, keeping 155100 word types\n",
      "2024-02-20 17:39:14,198 : INFO : PROGRESS: at sentence #920000, processed 16901578 words, keeping 156037 word types\n",
      "2024-02-20 17:39:14,247 : INFO : PROGRESS: at sentence #930000, processed 17090997 words, keeping 156964 word types\n",
      "2024-02-20 17:39:14,296 : INFO : PROGRESS: at sentence #940000, processed 17277575 words, keeping 157806 word types\n",
      "2024-02-20 17:39:14,344 : INFO : PROGRESS: at sentence #950000, processed 17463013 words, keeping 158737 word types\n",
      "2024-02-20 17:39:14,362 : INFO : collected 158916 word types from a corpus of 17503976 raw words and 952321 sentences\n",
      "2024-02-20 17:39:14,362 : INFO : Creating a fresh vocabulary\n",
      "2024-02-20 17:39:14,514 : INFO : FastText lifecycle event {'msg': 'effective_min_count=20 retains 24800 unique words (15.605728812706083%% of original 158916, drops 134116)', 'datetime': '2024-02-20T17:39:14.514124', 'gensim': '4.1.2', 'python': '3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-20 17:39:14,515 : INFO : FastText lifecycle event {'msg': 'effective_min_count=20 leaves 17100338 word corpus (97.69402106127203%% of original 17503976, drops 403638)', 'datetime': '2024-02-20T17:39:14.515474', 'gensim': '4.1.2', 'python': '3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-20 17:39:14,619 : INFO : deleting the raw counts dictionary of 158916 items\n",
      "2024-02-20 17:39:14,622 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2024-02-20 17:39:14,622 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 12868120.592538388 word corpus (75.3%% of prior 17100338)', 'datetime': '2024-02-20T17:39:14.622329', 'gensim': '4.1.2', 'python': '3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-20 17:39:14,893 : INFO : estimated required memory for 24800 words, 2000000 buckets and 50 dimensions: 426988096 bytes\n",
      "2024-02-20 17:39:14,893 : INFO : resetting layer weights\n",
      "2024-02-20 17:39:16,161 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-20T17:39:16.161758', 'gensim': '4.1.2', 'python': '3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n",
      "2024-02-20 17:39:16,161 : INFO : FastText lifecycle event {'msg': 'training model with 5 workers on 24800 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2024-02-20T17:39:16.161758', 'gensim': '4.1.2', 'python': '3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2024-02-20 17:39:17,179 : INFO : EPOCH 1 - PROGRESS: at 3.97% examples, 514096 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:18,178 : INFO : EPOCH 1 - PROGRESS: at 8.72% examples, 559783 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:19,194 : INFO : EPOCH 1 - PROGRESS: at 13.42% examples, 572556 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:20,194 : INFO : EPOCH 1 - PROGRESS: at 18.39% examples, 588078 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:21,208 : INFO : EPOCH 1 - PROGRESS: at 23.50% examples, 599324 words/s, in_qsize 10, out_qsize 0\n",
      "2024-02-20 17:39:22,210 : INFO : EPOCH 1 - PROGRESS: at 28.59% examples, 608189 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:23,227 : INFO : EPOCH 1 - PROGRESS: at 33.64% examples, 611510 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:24,239 : INFO : EPOCH 1 - PROGRESS: at 38.61% examples, 615028 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:25,245 : INFO : EPOCH 1 - PROGRESS: at 43.62% examples, 617552 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:26,246 : INFO : EPOCH 1 - PROGRESS: at 48.69% examples, 621058 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:27,255 : INFO : EPOCH 1 - PROGRESS: at 53.66% examples, 622453 words/s, in_qsize 10, out_qsize 0\n",
      "2024-02-20 17:39:28,258 : INFO : EPOCH 1 - PROGRESS: at 57.84% examples, 615410 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:29,277 : INFO : EPOCH 1 - PROGRESS: at 62.21% examples, 610868 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:30,289 : INFO : EPOCH 1 - PROGRESS: at 67.08% examples, 611245 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:31,293 : INFO : EPOCH 1 - PROGRESS: at 71.81% examples, 610810 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:32,305 : INFO : EPOCH 1 - PROGRESS: at 76.61% examples, 611044 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:33,306 : INFO : EPOCH 1 - PROGRESS: at 81.46% examples, 611783 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:34,324 : INFO : EPOCH 1 - PROGRESS: at 86.42% examples, 612686 words/s, in_qsize 10, out_qsize 0\n",
      "2024-02-20 17:39:35,342 : INFO : EPOCH 1 - PROGRESS: at 91.53% examples, 614427 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:36,357 : INFO : EPOCH 1 - PROGRESS: at 96.58% examples, 615690 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:37,009 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2024-02-20 17:39:37,025 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2024-02-20 17:39:37,026 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2024-02-20 17:39:37,044 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2024-02-20 17:39:37,048 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-02-20 17:39:37,056 : INFO : EPOCH - 1 : training on 17503976 raw words (12869928 effective words) took 20.9s, 616451 effective words/s\n",
      "2024-02-20 17:39:38,063 : INFO : EPOCH 2 - PROGRESS: at 4.82% examples, 619541 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:39,092 : INFO : EPOCH 2 - PROGRESS: at 9.75% examples, 618849 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:40,095 : INFO : EPOCH 2 - PROGRESS: at 14.72% examples, 623907 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:41,106 : INFO : EPOCH 2 - PROGRESS: at 19.76% examples, 628167 words/s, in_qsize 10, out_qsize 0\n",
      "2024-02-20 17:39:42,131 : INFO : EPOCH 2 - PROGRESS: at 24.76% examples, 627309 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:43,139 : INFO : EPOCH 2 - PROGRESS: at 29.67% examples, 627062 words/s, in_qsize 10, out_qsize 0\n",
      "2024-02-20 17:39:44,160 : INFO : EPOCH 2 - PROGRESS: at 34.48% examples, 622637 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:45,162 : INFO : EPOCH 2 - PROGRESS: at 39.13% examples, 620039 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:46,184 : INFO : EPOCH 2 - PROGRESS: at 43.80% examples, 616084 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:47,218 : INFO : EPOCH 2 - PROGRESS: at 48.57% examples, 614245 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:48,226 : INFO : EPOCH 2 - PROGRESS: at 52.97% examples, 609538 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:49,249 : INFO : EPOCH 2 - PROGRESS: at 57.72% examples, 608982 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:50,257 : INFO : EPOCH 2 - PROGRESS: at 62.63% examples, 610373 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:51,280 : INFO : EPOCH 2 - PROGRESS: at 67.31% examples, 608760 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:52,282 : INFO : EPOCH 2 - PROGRESS: at 71.93% examples, 607770 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:53,298 : INFO : EPOCH 2 - PROGRESS: at 76.61% examples, 606825 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:54,314 : INFO : EPOCH 2 - PROGRESS: at 81.34% examples, 606488 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:55,315 : INFO : EPOCH 2 - PROGRESS: at 85.95% examples, 605626 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:56,330 : INFO : EPOCH 2 - PROGRESS: at 90.61% examples, 604959 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:57,345 : INFO : EPOCH 2 - PROGRESS: at 95.08% examples, 603090 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:39:58,364 : INFO : EPOCH 2 - PROGRESS: at 99.67% examples, 602077 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-20 17:39:58,375 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2024-02-20 17:39:58,380 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2024-02-20 17:39:58,398 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2024-02-20 17:39:58,412 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2024-02-20 17:39:58,417 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-02-20 17:39:58,417 : INFO : EPOCH - 2 : training on 17503976 raw words (12868467 effective words) took 21.4s, 602511 effective words/s\n",
      "2024-02-20 17:39:59,439 : INFO : EPOCH 3 - PROGRESS: at 4.59% examples, 587767 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:00,463 : INFO : EPOCH 3 - PROGRESS: at 9.12% examples, 577093 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:01,463 : INFO : EPOCH 3 - PROGRESS: at 13.87% examples, 587206 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:02,467 : INFO : EPOCH 3 - PROGRESS: at 18.78% examples, 597442 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:03,468 : INFO : EPOCH 3 - PROGRESS: at 23.61% examples, 599696 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:04,499 : INFO : EPOCH 3 - PROGRESS: at 28.31% examples, 598465 words/s, in_qsize 8, out_qsize 1\n",
      "2024-02-20 17:40:05,500 : INFO : EPOCH 3 - PROGRESS: at 33.17% examples, 600939 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:06,512 : INFO : EPOCH 3 - PROGRESS: at 37.81% examples, 600190 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:07,505 : INFO : EPOCH 3 - PROGRESS: at 42.43% examples, 599345 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:08,532 : INFO : EPOCH 3 - PROGRESS: at 46.76% examples, 594243 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:09,529 : INFO : EPOCH 3 - PROGRESS: at 51.37% examples, 593937 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:10,551 : INFO : EPOCH 3 - PROGRESS: at 55.92% examples, 592810 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:11,567 : INFO : EPOCH 3 - PROGRESS: at 60.21% examples, 589117 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:12,599 : INFO : EPOCH 3 - PROGRESS: at 64.80% examples, 587642 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:13,601 : INFO : EPOCH 3 - PROGRESS: at 69.43% examples, 588077 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:14,601 : INFO : EPOCH 3 - PROGRESS: at 73.83% examples, 586657 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:15,618 : INFO : EPOCH 3 - PROGRESS: at 78.31% examples, 585891 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:16,631 : INFO : EPOCH 3 - PROGRESS: at 82.94% examples, 585952 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:17,639 : INFO : EPOCH 3 - PROGRESS: at 87.16% examples, 583500 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:18,631 : INFO : EPOCH 3 - PROGRESS: at 91.70% examples, 583624 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:19,647 : INFO : EPOCH 3 - PROGRESS: at 96.47% examples, 584436 words/s, in_qsize 10, out_qsize 0\n",
      "2024-02-20 17:40:20,344 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2024-02-20 17:40:20,362 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2024-02-20 17:40:20,366 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2024-02-20 17:40:20,366 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2024-02-20 17:40:20,392 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-02-20 17:40:20,392 : INFO : EPOCH - 3 : training on 17503976 raw words (12866556 effective words) took 22.0s, 585761 effective words/s\n",
      "2024-02-20 17:40:21,394 : INFO : EPOCH 4 - PROGRESS: at 4.54% examples, 583147 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:22,410 : INFO : EPOCH 4 - PROGRESS: at 9.40% examples, 602661 words/s, in_qsize 10, out_qsize 0\n",
      "2024-02-20 17:40:23,412 : INFO : EPOCH 4 - PROGRESS: at 14.32% examples, 610205 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:24,412 : INFO : EPOCH 4 - PROGRESS: at 19.20% examples, 613396 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:25,413 : INFO : EPOCH 4 - PROGRESS: at 23.95% examples, 612308 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:26,441 : INFO : EPOCH 4 - PROGRESS: at 28.53% examples, 606395 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:27,443 : INFO : EPOCH 4 - PROGRESS: at 33.52% examples, 609813 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:28,445 : INFO : EPOCH 4 - PROGRESS: at 38.21% examples, 609498 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:29,450 : INFO : EPOCH 4 - PROGRESS: at 43.00% examples, 609962 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:30,447 : INFO : EPOCH 4 - PROGRESS: at 47.79% examples, 610571 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:31,457 : INFO : EPOCH 4 - PROGRESS: at 52.64% examples, 611496 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:32,459 : INFO : EPOCH 4 - PROGRESS: at 57.17% examples, 609415 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:33,475 : INFO : EPOCH 4 - PROGRESS: at 61.93% examples, 609233 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:34,478 : INFO : EPOCH 4 - PROGRESS: at 66.57% examples, 608008 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:35,517 : INFO : EPOCH 4 - PROGRESS: at 70.97% examples, 603723 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:36,530 : INFO : EPOCH 4 - PROGRESS: at 75.46% examples, 601777 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:37,530 : INFO : EPOCH 4 - PROGRESS: at 80.16% examples, 601688 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:38,533 : INFO : EPOCH 4 - PROGRESS: at 84.70% examples, 600902 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:39,537 : INFO : EPOCH 4 - PROGRESS: at 89.48% examples, 601586 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:40,530 : INFO : EPOCH 4 - PROGRESS: at 94.06% examples, 600712 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:41,562 : INFO : EPOCH 4 - PROGRESS: at 98.88% examples, 601194 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:41,763 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2024-02-20 17:40:41,768 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2024-02-20 17:40:41,771 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2024-02-20 17:40:41,774 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2024-02-20 17:40:41,785 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-02-20 17:40:41,785 : INFO : EPOCH - 4 : training on 17503976 raw words (12868265 effective words) took 21.4s, 601714 effective words/s\n",
      "2024-02-20 17:40:42,785 : INFO : EPOCH 5 - PROGRESS: at 4.37% examples, 564011 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:43,799 : INFO : EPOCH 5 - PROGRESS: at 8.89% examples, 570586 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:44,797 : INFO : EPOCH 5 - PROGRESS: at 13.53% examples, 577997 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:45,822 : INFO : EPOCH 5 - PROGRESS: at 17.49% examples, 556457 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:46,832 : INFO : EPOCH 5 - PROGRESS: at 21.15% examples, 538483 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:47,847 : INFO : EPOCH 5 - PROGRESS: at 24.81% examples, 526271 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:48,849 : INFO : EPOCH 5 - PROGRESS: at 28.37% examples, 515732 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:49,851 : INFO : EPOCH 5 - PROGRESS: at 32.07% examples, 509541 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:50,861 : INFO : EPOCH 5 - PROGRESS: at 35.61% examples, 503504 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:51,874 : INFO : EPOCH 5 - PROGRESS: at 39.19% examples, 498715 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:52,870 : INFO : EPOCH 5 - PROGRESS: at 42.66% examples, 494126 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:53,905 : INFO : EPOCH 5 - PROGRESS: at 46.37% examples, 491457 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:54,932 : INFO : EPOCH 5 - PROGRESS: at 49.93% examples, 488280 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:55,938 : INFO : EPOCH 5 - PROGRESS: at 53.55% examples, 486051 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:56,948 : INFO : EPOCH 5 - PROGRESS: at 57.22% examples, 485012 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:57,957 : INFO : EPOCH 5 - PROGRESS: at 60.73% examples, 482895 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:58,967 : INFO : EPOCH 5 - PROGRESS: at 64.22% examples, 480735 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:40:59,962 : INFO : EPOCH 5 - PROGRESS: at 68.07% examples, 481315 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:41:00,971 : INFO : EPOCH 5 - PROGRESS: at 71.70% examples, 480566 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:41:01,991 : INFO : EPOCH 5 - PROGRESS: at 75.23% examples, 479040 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:41:02,993 : INFO : EPOCH 5 - PROGRESS: at 78.85% examples, 478210 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:41:04,022 : INFO : EPOCH 5 - PROGRESS: at 82.61% examples, 477785 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:41:05,031 : INFO : EPOCH 5 - PROGRESS: at 86.24% examples, 477451 words/s, in_qsize 10, out_qsize 0\n",
      "2024-02-20 17:41:06,032 : INFO : EPOCH 5 - PROGRESS: at 89.87% examples, 477119 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:41:07,023 : INFO : EPOCH 5 - PROGRESS: at 93.53% examples, 476793 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:41:08,035 : INFO : EPOCH 5 - PROGRESS: at 97.13% examples, 476236 words/s, in_qsize 9, out_qsize 0\n",
      "2024-02-20 17:41:08,777 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2024-02-20 17:41:08,815 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2024-02-20 17:41:08,820 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2024-02-20 17:41:08,823 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2024-02-20 17:41:08,836 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-02-20 17:41:08,836 : INFO : EPOCH - 5 : training on 17503976 raw words (12870171 effective words) took 27.0s, 475864 effective words/s\n",
      "2024-02-20 17:41:08,836 : INFO : FastText lifecycle event {'msg': 'training on 87519880 raw words (64343387 effective words) took 112.7s, 571067 effective words/s', 'datetime': '2024-02-20T17:41:08.836967', 'gensim': '4.1.2', 'python': '3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2024-02-20 17:41:09,485 : INFO : FastText lifecycle event {'params': 'FastText(vocab=24800, vector_size=50, alpha=0.025)', 'datetime': '2024-02-20T17:41:09.485783', 'gensim': '4.1.2', 'python': '3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from gensim.models import FastText\n",
    "\n",
    "num_feature = 50\n",
    "min_word_count = 20\n",
    "num_thread = 5\n",
    "window_size = 10\n",
    "down_sampling = 0.001\n",
    "iteration = 20\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model_fastText = FastText(sentences, \n",
    "                          vector_size=num_feature,  # Using vector_size instead of size\n",
    "                          window=window_size, \n",
    "                          min_count=min_word_count, \n",
    "                          workers=num_thread,\n",
    "                          min_n=3,  # Minimum length of char n-grams\n",
    "                          max_n=6)  # Maximum length of char n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to find similar words to `character`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('characterisation', 0.8808436989784241),\n",
       " ('characterize', 0.8790835738182068),\n",
       " ('characteristically', 0.8554302453994751),\n",
       " ('characterization', 0.8502060770988464),\n",
       " ('characteristic', 0.8450654745101929),\n",
       " ('protagonist', 0.837716281414032),\n",
       " ('uncharacteristically', 0.8267769813537598),\n",
       " ('characterized', 0.8229421973228455),\n",
       " ('characteristics', 0.8223468661308289),\n",
       " ('characterisations', 0.8144554495811462)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fastText.wv.most_similar('character')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the word `citi`, with the `word2vec` model, we can't find any similar words to it because it doesn't exist in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'citi' doesn't exist in word2vec vocab.\n"
     ]
    }
   ],
   "source": [
    "# Trying to find 'citi' word in the word2vec model vocb.\n",
    "try:\n",
    "    print(model.wv.get_vector('citi'))\n",
    "except KeyError:\n",
    "    print(\"Word 'citi' doesn't exist in word2vec vocab.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for word 'citi' in fastText vocab: \n",
      "[-0.35331836 -1.8534994  -1.687759   -3.1684115  -2.7455697  -0.14201069\n",
      "  1.5212997   5.6288514   1.2461898   0.2898962   1.9896507  -1.5007322\n",
      " -1.9811895   1.472321    1.8036091   0.928356   -0.7877016  -2.7003706\n",
      " -0.6201223   3.3970153  -0.707863   -0.30301785 -1.1019266  -0.35861874\n",
      " -1.58039     1.8841965   1.2614226  -0.0312026   2.7832553  -2.8458462\n",
      "  0.55815643  1.395603    1.077099   -2.1922326   2.1213596  -0.8281086\n",
      " -0.46570596 -1.0203273   0.18812998 -1.2443421  -1.8714466  -0.50708544\n",
      "  2.3123326   2.406491   -4.420427    0.11099686  1.2977874   1.8224685\n",
      "  1.0413573   2.5069156 ]\n"
     ]
    }
   ],
   "source": [
    "# Trying to find 'citi' word in the fastText model vocb.\n",
    "\n",
    "print(\"Vector for word 'citi' in fastText vocab: \\n{}\".format(model_fastText.wv.get_vector('citi')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `fastText` learns word representation better than `word2vec` because it breaks word into sub-words, making the learning more efficient. As a result, it can find better similiar words as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('citizens', 0.8885571956634521),\n",
       " ('citizen', 0.8578288555145264),\n",
       " ('europa', 0.8339913487434387),\n",
       " ('america', 0.8214999437332153),\n",
       " ('citys', 0.8171120882034302),\n",
       " ('europe', 0.8087190985679626),\n",
       " ('metropolitan', 0.803908109664917),\n",
       " ('civilian', 0.792152464389801),\n",
       " ('civil', 0.7896285057067871),\n",
       " ('civic', 0.786487877368927)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar words for the word 'citi' from the vocab list.\n",
    "\n",
    "model_fastText.wv.most_similar('citi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogy task for similar example as in `word2vec` model, the result is the same for the most similar word - `worst`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', 0.8682386875152588),\n",
       " ('dumbest', 0.8550361394882202),\n",
       " ('lamest', 0.803633451461792),\n",
       " ('laziest', 0.7880313396453857),\n",
       " ('ugliest', 0.7783506512641907),\n",
       " ('zest', 0.777234673500061),\n",
       " ('silliest', 0.7568947076797485),\n",
       " ('horst', 0.7552919983863831),\n",
       " ('tiniest', 0.7524278163909912),\n",
       " ('weakest', 0.7484068274497986)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar word from a negative word when give positive words and its respective negative word. example 1\n",
    "model_fastText.wv.most_similar(positive=['best','bad'],negative=['good'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, `fastText` model doesn't work well with analogy tasks in some cases (below example) whereas the opposite is true for `word2vec` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brynner', 0.8524536490440369),\n",
       " ('palmer', 0.8439511060714722),\n",
       " ('kavner', 0.8348411321640015),\n",
       " ('daphne', 0.8340691328048706),\n",
       " ('merlin', 0.8293867111206055),\n",
       " ('laura', 0.826379120349884),\n",
       " ('della', 0.8252083659172058),\n",
       " ('gabrielle', 0.8237994909286499),\n",
       " ('roscoe', 0.8233895897865295),\n",
       " ('darlene', 0.8228525519371033)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find most similar word from the list when given positive and respective negative words. example 2\n",
    "model_fastText.wv.most_similar(positive=['berlin','france'],negative=['germany'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different from the group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is similar with `word2vec` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "print(model_fastText.wv.doesnt_match(\"terrible bad horrible good\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the word vector and word index, syntax is the same when we do it with `word2vec` above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving model into array of words and vectors\n",
    "\n",
    "Finally, let's save the word vocabulary and their vectors for later RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model into array of words and vectors. \n",
    "vec_dim = 50\n",
    "word_list_fastText = []\n",
    "word_vector_fastText = np.zeros((len(model_fastText.wv.key_to_index) + 2, vec_dim)) # add '-' as padding character and 'unk' as unknown\n",
    "word_list_fastText.append('-') # padding value\n",
    "\n",
    "for i, word in enumerate(model_fastText.wv.index_to_key):\n",
    "    vector = model_fastText.wv.get_vector(word)\n",
    "    word_vector_fastText[i+1] = vector\n",
    "    word_list_fastText.append(word)\n",
    "\n",
    "word_list_fastText.append('unk')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of words in our vocabulary and vector shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in list: 24802\n",
      "Shape of word vector: (24802, 50)\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words in the list and its shape of the word vector.\n",
    "\n",
    "print(\"Number of words in list: {}\".format(len(word_list_fastText)))\n",
    "print(\"Shape of word vector: {}\".format(word_vector_fastText.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
